[
["index.html", "DSCI 551: Descriptive Statistics and Probability for Data Science Lecture Notes", " DSCI 551: Descriptive Statistics and Probability for Data Science Vincenzo Coia and Michael Gelbart 2019-10-07 Lecture Notes Lecture notes for DSCI 551 for the 2019/20 academic year. "],
["depicting-uncertainty.html", "Lecture 1 Depicting Uncertainty 1.1 Lecture Learning Objectives 1.2 Thinking about Probability 1.3 Probability Distributions", " Lecture 1 Depicting Uncertainty September 9, 2019 Welcome to the course! The syllabus is on the README of the _students repo. Today’s topics: probability, followed by distributions. 1.1 Lecture Learning Objectives From today’s lecture, students are expected to be able to: Identify probability as a proportion that converges to the truth as you collect more data. Calculate probabilities using the inclusion-exclusion principle, the law of total probability, and probability distributions. Convert between and interpret odds and probability. Specify the usefulness of odds over probability. Be aware that probability has multiple interpretations/philosophies. Calculate and interpret mean, mode, entropy, variance, and standard deviation, from both a distribution and a sample. (Hint: we make the quizzes based on lecture learning objectives) 1.2 Thinking about Probability 1.2.1 Defining Probability (5 min) I like to play Mario Kart 8, a racing game with some “combat” involved using items. In the game, you are given an item at random whenever you get an “item box”. Suppose you’re playing the game, and so far have gotten the following items in total: Item Name Count Banana 7 Bob-omb 3 Coin 37 Horn 1 Shell 2 Total: 50 Attribution: images from pngkey. Questions that we’ll address: What’s the probability that your next item is a coin? How would you find the actual probability? From this, how might you define probability? In general, the probability of an event \\(A\\) occurring is denoted \\(P(A)\\) and is defined as \\[\\frac{\\text{Number of times event } A \\text{ is observed}}{\\text{Total number of events observed}}\\] as the number of events goes to infinity. 1.2.2 Calculating Probabilities using Logic We’ll look at two laws for calculating probabilities of events. Suppose the table below show the true probabilities of each item. Also, let’s add some properties to these items. Item Name Probability Combat Type Defeats blue shells Banana 0.12 contact no Bob-omb 0.05 explosion no Coin 0.75 ineffective no Horn 0.03 explosion yes Shell 0.05 contact no Disclaimer: I don’t think these are the true probabilities, but I’m pretty sure the coin probability is correct, as long as you’re in the lead. 1.2.2.1 Law of Total Probability (5 min) According to this table, are there any other items possible? Why or why not? What’s the probability of getting something other than a coin? How did you arrive at that number? Concept: When partitioning the sample space (= the set of all possibilities), the probabilities of each piece should add to one. That is, in this case, \\[1 = P(\\text{Banana}) + P(\\text{Bob-omb}) + P(\\text{Coin}) + P(\\text{Horn}) + P(\\text{Shell}).\\] A special case of this involves the complement of an event. This partitions the sample space into two – for example, getting a coin or not a coin. For a general event \\(A\\), the law becomes: \\[1 = P(A) + P(\\neg A),\\] where \\(\\neg\\) means the complement (read “not”). 1.2.2.2 Inclusion-Exclusion (5 min) Let’s answer these questions by counting: 1. What’s the probability of getting an item that has an explosion combat type? 2. What’s the probability of getting an item that is both an explosion item and defeats blue shells? This is written \\(P(\\text{explosion} \\cap \\text{defeats blue shells})\\), where \\(\\cap\\) means “and”. 3. What’s the probability of getting an item that is an explosion item or an item that defeats blue shells? This is written \\(P(\\text{explosion} \\cup \\text{defeats blue shells})\\), where \\(\\cup\\) means “or”. In general, we can answer the third question with the inclusion-exclusion principle: for events \\(A\\) and \\(B\\), \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B).\\] We can extend this to three events, too: \\[P(A \\cup B \\cup C) = P(A) + P(B) + P(C) - P(A \\cap B) - P(B \\cap C) - P(A \\cap C) + P(A \\cap B \\cap C).\\] 1.2.3 Comparing Probabilities (8 min) True or False (2-3 min): Suppose Vincenzo often wins at a game of solitaire, but that Tom is twice as good as Vincenzo. This means that \\(P(\\text{Tom wins}) = 2 \\times P(\\text{Vincenzo wins})\\). Probability is quite useful for communicating the chance of an event happening in an absolute sense, but is not useful for comparing probabilities. Odds, on the other hand, are useful for comparing the chance of two events. If \\(p\\) is the chance that Vincenzo wins at solitaire, his odds of winning is defined as \\[\\text{Odds} = \\frac{p}{1-p}.\\] This means that, if his odds are \\(o\\), then the probability of winning is \\[\\text{Probability} = \\frac{o}{o+1}.\\] For example, if Vincenzo wins 80% of the time, his odds are \\(0.8/0.2 = 4\\). This is sometimes written as 4:1 odds – that is, four wins for every loss. If Tom is twice as good as Vincenzo, it’s most useful to say that this means Tom wins twice as many games before experiencing a loss (on average) – that is, 8:1 odds, or simply 8, and a probability of \\(8/9=0.888\\ldots\\). 1.2.4 Interpreting Probability (5 min) Thought experiment: What’s the probability of seeing a 6 after rolling a die? I roll a die, and cover the outcome. What’s the probability of seeing a 6 after I uncover the face? No philosophy is “wrong”! But why is this relevant in practice? It often doesn’t actually make sense to talk about the probability of an event, such as the probability that a patient has a particular disease. Instead, it’s a belief system that can be modified. It influences our choice of whether we choose a Bayesian or Frequentist analysis. More on this later in MDS. 1.3 Probability Distributions So far, we’ve been discussing probabilities of single events. But it’s often useful to characterize the full “spectrum” of uncertainty associated with an outcome. The set of all outcomes and their corresponding probabilities is called a probability distribution (or, often, just distribution). The outcome itself, which is uncertain, is called a random variable. (Note: technically, this definition only holds if the outcome is numeric, not categorical like our Mario Kart example, but we won’t concern ourselves with such details) When the outcomes are discrete, the distributions are called probability mass functions (or pmf’s for short). 1.3.1 Examples of Probability Distributions (3 min) Mario Kart Example: The distribution of items is given by the following table: Item Name Probability Banana 0.12 Bob-omb 0.05 Coin 0.75 Horn 0.03 Shell 0.05 The distribution of combat type is given by the following table: Combat Type Probability contact 0.17 explosion 0.08 ineffective 0.75 The distribution of defeating blue shells is given by the following table: Defeats blue shells Probability no 0.97 yes 0.03 Ship example (New): Suppose a ship that arrives at the port of Vancouver will stay at port according to the following distribution: Length of stay (days) Probability 1 0.25 2 0.50 3 0.15 4 0.10 The fact that the outcome is numeric means that there are more ways we can talk about things, as we will see. 1.3.2 Measures of central tendency and uncertainty (3 min) There are two concepts when communicating an uncertain outcome: Central tendency: a “typical” value of the outcome. Uncertainty: how “random” the outcome is. There are many ways to measure these two concepts. They’re defined using a probability distribution, but just as probability can be defined as the limit of a fraction based on a sample, these measures often have a sample version (aka empirical version) from which they are derived. As such, let’s call \\(X\\) the random outcome, and \\(X_1, \\ldots, X_n\\) a set of \\(n\\) observations that form a sample (see the terminology page for alternative uses of the word sample). 1.3.2.1 Mode and Entropy (5 min) No matter what scale a distribution has, we can always calculate the mode and entropy. And, when the outcome is categorical (like the Mario Kart example), we are pretty much stuck with these as our choices. The mode of a distribution is the outcome having highest probability. A measure of central tendency. The sample version is the observation you saw the most. Measured as an outcome, not as the probabilities. The entropy of a distribution is defined as \\[-\\displaystyle \\sum_x P(X=x)\\log(P(X=x)).\\] A measure of uncertainty. Probably the only measure that didn’t originate from a sample version (comes from information theory). Measured as a transformation of probabilities, not as the outcomes – so, hard to interpret on its own. Cannot be negative; zero-entropy means no randomness. 1.3.2.2 Mean and Variance (10 min) When our outcome is numeric, we can take advantage of the numeric property and calculate the mean and variance: The mean (aka expected value, or expectation) is defined as \\[\\displaystyle \\sum_x x\\cdot P(X=x).\\] A measure of central tendency, denoted \\(E(X)\\). Its sample version is \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i,\\) which gets closer and closer to the true mean as \\(n \\rightarrow \\infty\\) (this is in fact how the mean is originally defined!) Useful if you’re wanting to compare totals of a bunch of observations (just multiply the mean by the number of observations to get a sense of the total). Probably the most popular measure of central tendency. Note that the mean might not be a possible outcome! The variance is defined as \\[E[(X-E(X))^2],\\] or this works out to be equivalent to the (sometimes) more useful form, \\[E[X^2]-E[X]^2.\\] A measure of uncertainty, denoted \\(\\text{Var}(X)\\). Yes! This is an expectation – of the squared deviation from the mean. Its sample version is \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2\\), or sometimes \\(s^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2\\) – both get closer and closer to the true variance as \\(n \\rightarrow \\infty\\) (you’ll be able to compare the goodness of these at estimating the true variance in DSCI 552 next block). Like entropy, cannot be negative, and a zero variance means no randomness. Unlike entropy, depends on the actual values of the random variable. The standard deviation is the square root of the variance. Useful because it’s measured on the same scale as the outcome, as opposed to variance, which takes on squared outcome measurements. Note: you may have heard of the median – we’ll hold off on this until later. "],
["parametric-families.html", "Lecture 2 Parametric families 2.1 Learning Objectives 2.2 Properties of Distributions: Practice 2.3 Expectations of Transformations 2.4 Distribution Families 2.5 Common Distribution Families (12 min)", " Lecture 2 Parametric families September 11, 2019 2.1 Learning Objectives Identify probability as an expectation of a binary random variable. Calculate expectations of a linear combination of random variables. Match a physical process to a distribution family (Binomial, Geometric, Negative Binomial, Poisson, Bernoulli). Calculate probabilities, mean, and variance of a distribution belonging to a distribution family using either R or python. Distinguish between a family of distributions and a distribution. Identify whether a specification of parameters (such as mean and variance) is enough / too little / too much to specify a distribution from a family of distributions. 2.2 Properties of Distributions: Practice Let’s practice some of the concepts from last time. 2.2.1 Demonstration: Example computation (8 min) Let’s calculate the mean, variance, mode, and entropy of the following distribution (on the board). 2.2.2 Activity: Comparing Variance to Entropy (12 min) True or false? Plot A has higher entropy than Plot B. Plot A has higher variance than Plot B. Plot C has the highest possible variance amongst distributions with support {0, 1, 2, 3}. Plot D has the highest possible entropy amongst distributions with support {0, 1, 2, 3}. 2.3 Expectations of Transformations There are some properties of expectations that are particularly useful for some transformed random variables. 2.3.1 Linearity of Expectations (5 min) Expectations can be calculated simply under linear transformations. If \\(a\\) is a constant and \\(Y\\) is another random variable, then: \\(E[a X]=a E[X]\\) (also true for standard deviation) \\(E[X+Y]=E[X]+E[Y]\\) It does not mean \\(E[XY]=E[X]E[Y]\\) unless further assumptions are made; this is coming next week. It also does not mean \\(E(X^2)=E(X)^2\\), for instance. Example 1: The mean daily high temperature in Vancouver is 61 degrees Fahrenheit. What’s the mean temperature in Celcius? Remember, the conversion is \\(C = 5/9(F − 32)\\). Example 3: You’ll see in DSCI 562 that sometimes it makes more sense to calculate the mean of a logarithm. Suppose \\(E(\\log(X)) = 1.5\\). Is it true that \\(E(X) = \\exp(1.5) \\approx 4.48\\)? 2.3.2 Probability as an Expectation (3 min) Machine learning techniques are typically built to calculate expectations. But what if you want to calculate a probability? Luckily, probability can be defined as an expectation! Specifically, if you want to find out the probability of some event \\(A\\), this is simply the expectation of the following binary random variable: \\[ X = \\begin{cases} 0 \\text{ if } A \\text{ does not happen}, \\\\ 1 \\text{ if } A \\text{ does happen} \\end{cases} \\] That is, \\[P(A) = E(X).\\] 2.4 Distribution Families So far in our discussion of distributions, we’ve been talking about properties of distributions in general. Again, this is important because a huge component of data science is in attempting to describe an uncertain outcome, like the number of ships that arrive to the port of Vancouver on a given day, or the identity of a rock. There are some common processes that give rise to probability distributions having a very specific form, and these distributions are very useful in practice. Let’s use the Binomial family of distributions as an example. 2.4.1 Binomial Distribution (8 min) Process: Suppose you play a game, and win with probability \\(p\\). Let \\(X\\) be the number of games you win after playing \\(N\\) games. \\(X\\) is said to have a Binomial distribution, written \\(X \\sim \\text{Binomial} (N, p)\\). Example: (Demonstration on the board) Let’s derive the probability of winning exactly two games out of three. That is, \\(P(X=2)\\) when \\(N=3\\). pmf: A binomial distribution is characterized by the following pmf: \\[P(X=x) = {N \\choose x} p^x (1-p)^{N-x}.\\] Remember, \\(N \\choose x\\) is read as “N choose x”. You can think of it as the number of ways you can make a team of \\(x\\) people from a total of \\(N\\) people. You can calculate this in R with choose(N, x), and its formula is \\[{N \\choose x} = \\frac{N!}{x!(N-x)!}.\\] mean: \\(Np\\) variance: \\(Np(1-p)\\) Code: The pmf can be calculated in R with dbinom(); in python, scipy.stats.binom. Here is an example pmf for a Binomial(N = 5, p = 0.2) distribution: 2.4.2 Families vs. distributions (3 min) Specifying a value for both \\(p\\) and \\(N\\) results in a unique Binomial distribution. For example, the Binomial(N = 5, p = 0.2) distribution is plotted above. It’s therefore helpful to remember that there are in fact many Binomial distributions (actually infinite), one for each choice of \\(p\\) and \\(N\\). We refer to the entire set of probability distributions as the Binomial family of distributions. This means that it doesn’t actually make sense to talk about “the” Binomial distribution! This is important to remember as we add on concepts throughout MDS, such as the maximum likelihood estimator that you’ll see in a future course. 2.4.3 Parameters (5 min) True or false: For a distribution with possible values {0, 1, 2, 3, 4, 5}, five probabilities need to be specified in order to fully describe the distribution. For a Binomial distribution with \\(N=5\\), five probabilities need to be specified in order to fully describe the distribution. Knowing \\(p\\) and \\(N\\) is enough to know the entire distribution within the Binomial family. That is, no further information is needed – we know all \\(N+1\\) probabilities based on only two numbers! Since \\(p\\) and \\(N\\) fully specify a Binomial distribution, we call them parameters of the Binomial family, and we call the Binomial family a parametric family of distributions. In general, a parameter is a variable whose specification narrows down the space of possible distributions (or to be even more general, the space of possible models). 2.4.4 Parameterization (8 min) A Binomial distribution can be specified by knowing \\(N\\) and \\(p\\), but there are other ways we can specify the distribution. For instance, specifying the mean and variance is enough to specify a Binomial distribution. Demonstration: Which Binomial distribution has mean 2 and variance 1? (on the whiteboard) Exactly which variables we decide to use to identify a distribution within a family is called the family’s parameterization. So, the Binomial distribution is usually parameterized according to \\(N\\) and \\(p\\), but could also be parameterized in terms of the mean and variance. The “usual” parameterization of a distribution family is sometimes called the canonical parameterization. In general, there are many ways in which a distribution family can be parameterized. The parameterization you use in practice will depend on the information you can more easily obtain. 2.4.5 Distribution Families in Practice Why is it useful to know about distribution families? In general when we’re modelling something, like river flow or next month’s net gains or the number of ships arriving at port tomorrow, you have the choice to make a distributional assumption or not. That is, do you want to declare the random variable of interest as belonging to a certain distribution family, or do you want to allow the random variable to have a fully general distribution? Both are good options depending on the scenario, and later in the program, we’ll explore the tradeoff with both options in more detail. 2.5 Common Distribution Families (12 min) Aside from the Binomial family of distributions, there are many other families that come up in practice. Here are some of them. For a more complete list, check out Wikipedia’s list of probability distributions. In practice, it’s rare to encounter situations that are exactly described by a distribution family, but distribution families still act as useful approximations. Details about these distributions are specified abundantly online. My favourite resource is Wikipedia, which organizes a distribution family’s properties in a fairly consistent way – for example here is the page on the Binomial family. We won’t bother transcribing these details here, but instead focus on some highlights. 2.5.1 Geometric Process: Suppose you play a game, and win with probability \\(p\\). Let \\(X\\) be the number of attempts at playing the game before experiencing a win. Then \\(X\\) is said to have a Geometric distribution. Note: Sometimes this family is defined so that \\(X\\) includes the winning attempt. The properties of the distribution differ, so be sure to be deliberate about which one you use. Since there’s only one parameter, this means that if you know the mean, you also know the variance! Code: The pmf can be calculated in R with dgeom(); in python, scipy.stats.geom. 2.5.2 Negative Binomial Process: Suppose you play a game, and win with probability \\(p\\). Let \\(X\\) be the number of attempts at playing the game before experiencing \\(k\\) wins. Then \\(X\\) is said to have a Negative Binomial distribution. Two parameters. The Geometric family results with \\(k=1\\). Code: The pmf can be calculated in R with dnb(); in python, scipy.stats.nbinom. 2.5.3 Poisson Process: Suppose customers independently arrive at a store at some average rate. The total number of customers having arrived after a pre-specified length of time follows a Poisson distribution, and can be parameterized by a single parameter, usually the mean \\(\\lambda\\). A noteable property of this family is that the mean is equal to the variance. Examples that are indicative of this process: The number of ships that arrive at the port of Vancouver in a given day. The number of emails you receive in a given day. Code: The pmf can be calculated in R with dpois(); in python, scipy.stats.poisson. 2.5.4 Bernoulli A random variable that is either \\(1\\) (with probability \\(p\\)) or \\(0\\) (with probability \\(1-p\\)). Parameterized by \\(p\\). A special case of the Binomial family, with \\(N=1\\). "],
["simulation.html", "Lecture 3 Simulation 3.1 Learning Objectives 3.2 Review Activity (15 min) 3.3 Random Samples: Terminology (5 min) 3.4 Seeds (5 min) 3.5 Generating Random Samples: Code 3.6 Running Simulations", " Lecture 3 Simulation So far, we’ve seen many quantities that help us communicating an uncertain outcome: probability probability mass function odds mode entropy mean variance / standard deviation Sometimes, it’s not easy to compute these things. In these situations, we can use simulation to approximate these and other quantities. This is today’s topic. Let’s set up the workspace for this lecture: suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(reticulate)) suppressPackageStartupMessages(library(testthat)) ## Python: import numpy as np import scipy.stats 3.1 Learning Objectives From this lecture, students are expected to be able to: Generate a random sample from a discrete distribution in both R and python. Reproduce the same random sample each time you re-run your code in both R and python by setting the seed or random state. Evaluate whether or not a set of observations are iid. Use simulation to approximate distribution properties (like mean and variance) using empirical quantities, especially for random variables involving multiple other random variables. 3.2 Review Activity (15 min) True or False? In general, 9 parameters must be specified in order to fully describe a distribution with 9 outcomes. A Binomial distribution only has one mean, but there are many Binomial distributions that have the same mean. A Poisson distribution only has one mean, but there are many Poisson distributions that have the same mean. A Binomial distribution is also a Bernoulli distribution, but a Bernoulli distribution is not a Binomial distribution. 3.3 Random Samples: Terminology (5 min) A random sample is a collection of random outcomes/variables. Using symbols, a random sample of size \\(n\\) is usually depicted as \\(X_1, \\ldots, X_n\\). We think of data as being a random sample. Some examples of random samples: the first five items you get in a game of Mario Kart the outcomes of ten dice rolls the daily high temperature in Vancouver for each day in a year. A random sample is said to be independent and identically distributed (or iid) if each pair of observations are independent, and each observation comes from the same distribution. We’ll define “independent” next class, but for now, you can think of this as meaning “not influencing each other”. Sometimes, when an outcome is said to be random, this can either mean the outcome has some distribution (with non-zero entropy), or that is has the distribution with maximum entropy. To avoid confusion, the word stochastic refers to the former (as having some uncertain outcome). For example, if a die is weighted so that “1” appears very often, would you call this die “random”? Whether or not you do, it’s always stochastic. The opposite of stochastic is deterministic: an outcome that will be known with 100% certainty. 3.4 Seeds (5 min) Computers can’t actually generate truly random outcomes. Instead, they use something called pseudorandom numbers. As an example of a basic algorithm that produces pseudo-random numbers between 0 and 1, consider starting with your choice of number \\(x_0\\) between 0 and 1, and iterating the following equation: \\[x_{i+1} = 4 x_i (1 - x_i).\\] The result will appear to be random numbers between 0 and 1. Here is the resulting sequence when we start with \\(x_0 = 0.3\\) and iterate 1000 times: Although this sequence is deterministic, it behaves like a random sample. But not entirely! All pseudorandom number generators have some pitfalls. In the case above, one pitfall is that neighbouring pairs are not independent from each other (by definition of the way the sequence was set up!). There are some sophisticated algorithms that produce outcomes that more closely resemble a random sample, so most of the time, we don’t have to worry about the sample not being truly random. The seed (or random state) in a pseudo-random number generator is some pre-specified initial value that determines the generated sequence. As long as the seed remains the same, the resulting sample will also be the same. In the case above, this is \\(x_0 = 0.3\\). In R and python, if we don’t explicitly set the seed, then the seed will be chosen for us. In R, we can set the seed using the set.seed() function, and in python, using the numpy.random.seed() function from numpy. The seed gives us an added advantage over truly random numbers: it allows our analysis to be reproducible! If we explicitly set a seed, then someone who re-runs the analysis will get the same results. 3.5 Generating Random Samples: Code Here, we’ll look at some R and python functions that help us generate a random sample. We’re still focussing on discrete distributions, here. 3.5.1 From Finite Number of Categories (5 min) In R, we can generate a random sample from a distribution with a finite number of outcomes using the sample() function: Put the outcomes as a vector in the first argument, x. Put the desired sample size in the argument size. Put replace = TRUE so that sampling can happen with replacement. Put the probabilities of the outcomes as a vector respective to x in the argument prob. Just a warning: if these probabilities do not add up to 1, R will not throw an error. Instead, R automatically adjusts the probabilities so that they add up to 1. Here’s an example of generating 10 items using the Mario Kart item distribution from Lecture 1. Notice that the seed is set, so that every time these lecture notes are rendered, the same results are obtained. set.seed(1) outcomes &lt;- c(&quot;banana&quot;, &quot;bob-omb&quot;, &quot;coin&quot;, &quot;horn&quot;, &quot;shell&quot;) probs &lt;- c(0.12, 0.05, 0.75, 0.03, 0.05) n &lt;- 10 sample(outcomes, size = n, replace = TRUE, prob = probs) ## [1] &quot;coin&quot; &quot;coin&quot; &quot;coin&quot; &quot;bob-omb&quot; &quot;coin&quot; &quot;bob-omb&quot; &quot;shell&quot; ## [8] &quot;coin&quot; &quot;coin&quot; &quot;coin&quot; In python, we can generate a random sample from a discrete distribution using the numpy.random.choice() function: Put the outcomes in the first argument, a. Put the desired sample size in the argument size. Put the probabilities of the outcomes respective to x in the argument p. Using the Mario Kart example again: ## Python: np.random.seed(1) outcomes = [&quot;banana&quot;, &quot;bob-omb&quot;, &quot;coin&quot;, &quot;horn&quot;, &quot;shell&quot;] probs = [0.12, 0.05, 0.75, 0.03, 0.05] n = 10 np.random.choice(outcomes, size = n, p = probs) ## array([&#39;coin&#39;, &#39;coin&#39;, &#39;banana&#39;, &#39;coin&#39;, &#39;bob-omb&#39;, &#39;banana&#39;, &#39;coin&#39;, ## &#39;coin&#39;, &#39;coin&#39;, &#39;coin&#39;], ## dtype=&#39;&lt;U7&#39;) 3.5.2 From Distribution Families (5 min) In R, we can generate data from a distribution belonging to some parametric family using the rdist() function, where “dist” is replaced with a short-form of the distribution family’s name. We can access the corresponding pmf with ddist(). In python, we can use the stats module from the scipy library. The following table summarizes the functions related to the distribution famlies we’ve seen so far: Family R function python function Binomial rbinom() scipy.stats.binom.rvs() Geometric rgeom() scipy.stats.geom.rvs() Negative Binomial rnbinom() scipy.stats.nbinom.rvs() Poisson rpois() scipy.stats.poisson.rvs() Here’s how to use these functions: Sample size: For R, put this in the argument n, which comes first. For python, put this in the argument size, which comes last. In both languages, each parameter has its own argument. Sometimes, like in R’s rnbinom(), there are more parameters than needed, giving the option of different parameterizations. Be sure to only specify the exact number of parameters required to isolate a member of the distribution family! Example: Generate 10 observations from a binomial distribution with probability of success 0.6 and 5 trials. Using R: rbinom(10, size = 5, prob = 0.6) ## [1] 4 4 2 3 2 3 2 0 3 2 Using python: ## Python: scipy.stats.binom.rvs(n = 5, p = 0.6, size = 10) ## array([3, 2, 4, 2, 5, 3, 3, 3, 4, 4]) The Negative Binomial family is an example of a function in R that allows for a different parameterization. Notice that specifying too many or too few parameters results in an error (remember, we need to specify two parameters): rnbinom(10, size = 5) ## Error in rnbinom(10, size = 5): argument &quot;prob&quot; is missing, with no default rnbinom(10, size = 5, prob = 0.6, mu = 4) ## Error in rnbinom(10, size = 5, prob = 0.6, mu = 4): &#39;prob&#39; and &#39;mu&#39; both specified 3.6 Running Simulations So far, we’ve seen two ways to calculate quantities that help us communicate uncertainty (like means and probabilities): The distribution-based approach (using the distribution), resulting in true values. The empirical approach (using data), resulting in approximate values that improve as the sample size increases. For example, the true mean of a random variable \\(X\\) can be calculated as \\(E(X) = \\sum_x x P(X = x)\\) using each pair of outcome and outcome’s probability, or can be approximated using the empirical approach from a random sample \\(X_1, \\ldots, X_n\\) by \\(E(X) \\approx (1/n) \\sum_{i=1}^n X_i\\). This means that we can approximate these quantities by generating a sample! An analysis that uses a randomly generated data set is called a simulation. 3.6.1 Code for empirical quantities (0 min) For your reference, here are some hints for calculating empirical quantities using R. We’ll be going over these below in the “Basic Simulation” section. mean() calculates the sample average. var() calculates the sample variance (the \\(n-1\\) version, not \\(n\\)), and sd() its square root for the standard deviation. For a single probability, remember that a mean is just an average. Just calculate the mean of a condition. For an entire pmf, use the table() function, or more conveniently, the janitor::tabyl() function. For the mode, either get it manually using the table() or janitor::tabyl() function, or you can use DescTools::Mode(). 3.6.2 Basic Simulation (10 min) Consider playing games with probability of success \\(p=0.7\\) until you experience \\(k=5\\) successes, and counting the number of failures. This random variable (say \\(X\\)) has a Negative Binomial distribution. You can find an R script containing the code for the basic simulation in the students’ repo. Let’s demonstrate both a distribution-based and empirical approach to computing the variance and pmf. First, let’s obtain our random sample (of, say, 10000 observations). set.seed(88) k &lt;- 5 p &lt;- 0.7 n &lt;- 10000 rand_sample &lt;- rnbinom(n, size = 5, prob = 0.7) head(rand_sample, 100) ## [1] 1 1 6 2 0 3 3 3 2 1 2 5 1 1 1 3 1 1 1 2 2 1 1 7 1 1 3 5 0 4 0 5 1 1 4 ## [36] 1 1 1 2 6 3 2 5 3 1 2 0 2 2 1 1 4 0 0 5 5 2 7 0 0 1 0 3 1 3 2 0 2 2 0 ## [71] 3 1 0 5 4 0 1 3 2 1 2 1 1 2 2 1 0 1 4 4 2 2 4 1 2 4 3 4 1 1 3.6.2.1 Mean (1 - p) * k / p # True, distribution-based ## [1] 2.142857 mean(rand_sample) # Approximate, empirical ## [1] 2.1654 3.6.2.2 Variance (1 - p) * k / p^2 # True, distribution-based ## [1] 3.061224 var(rand_sample) # Approximate, empirical ## [1] 3.060549 3.6.2.3 Standard deviation sqrt((1 - p) * k / p^2) # True, distribution-based ## [1] 1.749636 sd(rand_sample) # Approximate, empirical ## [1] 1.749442 3.6.2.4 Probability of seeing 0 mean(rand_sample == 0) # Approximate, empirical ## [1] 0.163 dnbinom(0, size = k, prob = p) # True, distribution-based ## [1] 0.16807 3.6.2.5 pmf ## Code without using the tidyverse: pmf &lt;- janitor::tabyl(rand_sample) # Empirical pmf$n &lt;- NULL pmf &lt;- setNames(pmf, c(&quot;x&quot;, &quot;empirical&quot;)) pmf$actual &lt;- dnbinom(pmf$x, size = k, prob = p) # True ## Code using the tidyverse: pmf &lt;- janitor::tabyl(rand_sample) %&gt;% select(x = rand_sample, empirical = percent) %&gt;% mutate(actual = dnbinom(x, size = k, prob = p)) pmf %&gt;% mutate(actual = round(actual, 4), # Empirical empirical = round(empirical, 4)) %&gt;% # True DT::datatable(rownames = FALSE) Here’s a plot of the pmf: pmf %&gt;% gather(key = &quot;method&quot;, value = &quot;Probability&quot;, empirical, actual) %&gt;% ggplot(aes(x, Probability)) + facet_wrap(~ method) + geom_col(fill = &quot;maroon&quot;) + theme_bw() 3.6.2.6 Entropy It turns out to be hard to calculate the actual entropy, so we will only compute the empirical: - sum(pmf$empirical * log(pmf$empirical)) ## [1] 1.858891 3.6.2.7 Mode feel free to just read from the table/plot if you aren’t familiar with the R tidyverse. ## Actual pmf %&gt;% filter(actual == max(actual)) %&gt;% pull(x) ## [1] 1 ## Empirical pmf %&gt;% filter(empirical == max(empirical)) %&gt;% pull(x) ## [1] 1 3.6.2.8 Distribution-based calculations on empirical pmf What do you think you’ll get if you use the definition of mean, variance, etc. on the empirical distribution? You get the empirical values! Here’s an example with the mean – notice that they are identical. sum(pmf$x * pmf$empirical) ## [1] 2.1654 mean(rand_sample) ## [1] 2.1654 3.6.2.9 Law of Large Numbers To demonstrate that the a larger sample size improves the approximation of the empirical quantities, let’s see how the sample average changes as we collect more and more data: tibble(i = 1:n, mean = cumsum(rand_sample) / i) %&gt;% ggplot(aes(i, mean)) + geom_hline(yintercept = (1 - p) * k / p, colour = &quot;maroon&quot;) + geom_line() + labs(x = &quot;Sample size&quot;, y = &quot;Empirical mean&quot;) + theme_bw() You can try this for yourself with Chapter 1 -“expectation” in Seeing Theory. 3.6.3 Multi-Step Simulations (10 min) The simulation above was not all that useful, since we could calculate basically anything. Where it gets more interesting is when we want to calculate things for a random variable that transforms and/or combines multiple random variables. The idea is that some random variables will have a distribution that depends on other random variables, but in a way that’s explicit. For example, consider a random variable \\(T\\) that we can obtain as follows. Take \\(X \\sim \\text{Poisson}(5)\\), and then \\(T = \\sum_{i = 1}^{X} D_i\\), where each \\(D_i\\) are iid with some specified distribution. In this case, to generate \\(T\\), you would first need to generate \\(X\\), then generate \\(X\\) values of \\(D_i\\), then sum those up to get \\(T\\). This is the example we’ll see here, but in general, you can have any number of dependencies, each component of which you would have to generate. Consider an example that a Vancouver port faces with “gang demand”. Whenever a ship arrives to the port of Vancouver, they request a certain number of “gangs” (groups of people) to help unload the ship. Let’s suppose the number of gangs requested by a ship has the following distribution: gang &lt;- 1:4 p &lt;- c(0.2, 0.4, 0.3, 0.1) tibble( gangs = gang, p = p ) %&gt;% ggplot(aes(gangs, p)) + geom_col(fill = &quot;maroon&quot;) + labs(x = &quot;Number of Gangs&quot;, y = &quot;Probability&quot;) + theme_bw() The following function sums up simulated gangs requested by a certain number of ships, with the above probability distribution as a default. As an example, check out the simulated gang request from 10 ships: #&#39; Generate gang demand #&#39; #&#39; Simulates the number of gangs requested, if each ship #&#39; requests a random number of gangs. #&#39; #&#39; @param n_ships Number of ships that are making demands #&#39; @param gangs Possible gang demands made by a ship. #&#39; @param prob Probabilities of gang demand corresponding to &quot;gangs&quot; #&#39; #&#39; @return Number representing the total gang demand demand_gangs &lt;- function(n_ships, gangs = gang, prob = p) { if (length(gangs) == 1) { gangs &lt;- c(gangs, gangs) prob &lt;- c(1,1) } requests &lt;- sample( gangs, size = n_ships, replace = TRUE, prob = prob ) sum(requests) } test_that(&quot;demand_gangs output is as expected&quot;, { expect_identical(demand_gangs(0), 0L) expect_gte(demand_gangs(1), min(gang)) expect_lte(demand_gangs(1), max(gang)) expect_gte(demand_gangs(10), 10*min(gang)) expect_lte(demand_gangs(10), 10*max(gang)) expect_identical(length(demand_gangs(10)), 1L) expect_identical(demand_gangs(10, gangs = 2, prob = 1), 20) }) demand_gangs(10) ## [1] 22 Now suppose that the number of ships that arrive on a given day follows the Poisson distribution with a mean of 5. What’s the distribution of total gang request on a given day? Let’s simulate the process to find out: Generate arrival quantities for many days from the Poisson(5) distribution. For each day, simulate total gang request for the simulated number of ships. You now have your random sample – compute things as you normally would. Let’s try this, obtaining a sample of 10000 days: n_days &lt;- 10000 ## Step 1: generate a bunch of ships arriving each day arrivals &lt;- rpois(n_days, lambda = 5) ## Step 2: Simulate total gang request on each day. total_requests &lt;- purrr::map_int(arrivals, demand_gangs) ## Step 3: Compute things like pmf, mean, variance: tibble(x = total_requests) %&gt;% ggplot(aes(x, y = ..prop..)) + geom_bar() + labs(x = &quot;Total Gang Request&quot;, y = &quot;Probability&quot;) + theme_bw() tibble( mean = mean(total_requests), variance = var(total_requests) ) %&gt;% knitr::kable() mean variance 11.5162 30.47739 3.6.4 Mixture distributions Moved to a future lecture. "],
["joint-probability-part-i.html", "Lecture 4 Joint Probability, Part I 4.1 Learning Objectives 4.2 Conditional Distributions (15 min) 4.3 Joint Distributions (25 min) 4.4 Dependence concepts", " Lecture 4 Joint Probability, Part I Today’s topic is on working with multiple variables – for now, just two. Set up the workspace: suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(pbivnorm)) suppressPackageStartupMessages(library(cowplot)) suppressPackageStartupMessages(library(DT)) knitr::opts_chunk$set(fig.width = 5, fig.height = 2, fig.align = &quot;center&quot;) 4.1 Learning Objectives From today’s class, students are expected to be able to: Calculate conditional distributions when giving a full distribution. Calculate marginal distributions from a joint distribution. Obtain the marginal mean from conditional means and marginal probabilities, using the law of total expectation. Use the law of total probability to convert between conditional + marginal distributions, and joint distributions. Describe the consequences of independent random variables. Calculate and describe the pros and cons of dependence measures: covariance, correlation, and kendall’s tau. 4.2 Conditional Distributions (15 min) Probability distributions describe an uncertain outcome, but what if we have partial information? Consider the example of ships arriving at the port of Vancouver again. Each ship will stay at port for a random number of days, which we’ll call the length of stay (LOS) or \\(D\\), according to the following (made up) distribution: Length of Stay (LOS) Probability 1 0.25 2 0.35 3 0.20 4 0.10 5 0.10 Suppose a ship has been at port for 2 days now, and it’ll be staying longer. What’s the distribution of length-of-stay now? Using symbols, this is written as \\(P(D = d \\mid D &gt; 2)\\), where the bar “|” reads as “given” or “conditional on”, and this distribution is called a conditional distribution. We can calculate a conditional distribution in two ways: a “table approach” and a “formula approach”. Table approach: Subset the pmf table to only those outcomes that satisfy the condition (\\(D &gt; 2\\) in this case). You’ll end up with a “sub table”. Re-normalize the remaining probabilities so that they add up to 1. You’ll end up with the conditional distribution under that condition. Formula approach: In general for events \\(A\\) and \\(B\\), the conditional probability formula is \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\\] For the ship example, the event \\(A\\) is \\(D = d\\) (for all possible \\(d\\)’s), and the event \\(B\\) is \\(D &gt; 2\\). Plugging this in, we get \\[P(D = d \\mid D &gt; 2) = \\frac{P(D = d \\cap D &gt; 2)}{P(D &gt; 2)} = \\frac{P(D = d)}{P(D &gt; 2)} \\text{ for } d = 3,4,5.\\] The only real “trick” is the numerator. How did we reduce the convoluted event \\(D = d \\cap D &gt; 2\\) to the simple event \\(D = d\\) for \\(d = 3,4,5\\)? The trick is to go through all outcomes and check which ones satisfy the requirement \\(D = d \\cap D &gt; 2\\). This reduces to \\(D = d\\), as long as \\(d = 3,4,5\\). 4.3 Joint Distributions (25 min) So far we’ve only considered one random variable at a time. Its distribution is called univariate because there’s just one variable. But, we very often have more than one random variable. Let’s start by considering two independent fair coins. The possibilities are: HH, HT, TH, TT, each with probability \\(0.25\\). We can visualize this as a joint distribution: H T H 0.25 0.25 T 0.25 0.25 Note that an outcome consists of a pair of random variables. The sum of all probabilities still add to 1, since this, too, is a probability distribution. We could define the first coin’s outcome as the \\(X\\) and the second as \\(Y\\) and write \\(P(X=H,Y=H)=0.25\\). Don’t be fooled, though! This is not really any different from what we’ve already seen. We can still write this a univariate distribution with four categories. This is useful to remember when we’re calculating probabilities. Outcome Probability HH 0.25 HT 0.25 TH 0.25 TT 0.25 Viewing the distribution as a (2-dimensional) matrix instead of a (1-dimensional) vector turns out to be more useful when determining properties of individual random variables. 4.3.1 Example: Length of Stay vs. Gang Demand Throughout today’s class, we’ll be working with the following joint distribution of length of stay of a ship, and its gang demand. Gangs = 1 Gangs = 2 Gangs = 3 Gangs = 4 LOS = 1 0.0017 0.0425 0.1247 0.0811 LOS = 2 0.0266 0.1698 0.1360 0.0176 LOS = 3 0.0511 0.1156 0.0320 0.0013 LOS = 4 0.0465 0.0474 0.0059 0.0001 LOS = 5 0.0740 0.0246 0.0014 0.0000 The joint distribution is stored in “tidy format” in an R variable named j: DT::datatable(j, rownames = FALSE) 4.3.2 Marginal Distributions We’ve just specified a joint distribution of length of stay and gang request. But, we’ve previously specified a distribution for these variables individually. These are not things that can be specified separately: If you have a joint distribution, then the distribution of each individual variable follows as a consequence. If you have the distribution of each individual variable, you still don’t have enough information to form the joint distribution between the variables. The distribution of an individual variable is called the marginal distribution (sometimes just “marginal” or “margin”). The word “marginal” is not really needed when we’re talking about a random variable – there’s no difference between the “marginal distribution of length of stay” and the “distribution of length of stay”, we just use the word “marginal” if we want to emphasize the distribution is being considered in isolation from other related variables. 4.3.3 Calculating Marginals from the Joint There’s no special way of calculating a marginal distribution from a joint distribution. As usual, it just involves adding up the probabilities corresponding to relevant outcomes. For example, to compute the marginal distribution of length of stay (LOS), we’ll first need to calculate \\(P(\\text{LOS} = 1)\\). Using the joint distribution of length of stay and gang request, the outcomes that satisfy this requirement are the entire first row of the probability table. It follows that the marginal distribution of LOS can be obtained by adding up each row. For the marginal of gang requests, just add up the columns. Here’s the marginal of LOS (don’t worry about the code, you’ll learn more about this code in DSCI 523 next block). Notice that the distribution of LOS is the same as before! j %&gt;% group_by(los) %&gt;% summarize(p = sum(p)) %&gt;% knitr::kable(col.names = c(&quot;Length of Stay&quot;, &quot;Probability&quot;)) Length of Stay Probability 1 0.25 2 0.35 3 0.20 4 0.10 5 0.10 Similarly, the distribution of gang request is the same as from last lecture: j %&gt;% group_by(gang) %&gt;% summarize(p = sum(p)) %&gt;% knitr::kable(col.names = c(&quot;Gang request&quot;, &quot;Probability&quot;)) Gang request Probability 1 0.2 2 0.4 3 0.3 4 0.1 4.3.4 Conditioning on one Variable What’s usually more interesting than a joint distribution are conditional distributions, when other variables are fixed. This is a special type of conditional distribution and an extremely important type of distribution in data science. For example, a ship is arriving, and they’ve told you they’ll only be staying for 1 day. What’s the distribution of their gang demand under this information? That is, what is \\(P(\\text{gang} = g \\mid \\text{LOS} = 1)\\) for all possible \\(g\\)? Table approach: Isolating the outcomes satisfying the condition (\\(\\text{LOS} = 1\\)), we obtain the first row: Gangs: 1 Gangs: 2 Gangs: 3 Gangs: 4 0.0017 0.0425 0.1247 0.0811 Now, re-normalize the probabilities so that they add up to 1, by dividing them by their sum, which is 0.25: Gangs: 1 Gangs: 2 Gangs: 3 Gangs: 4 0.0068 0.1701 0.4988 0.3243 Formula Approach: Applying the formula for conditional probabilities, we get \\[P(\\text{gang} = g \\mid \\text{LOS} = 1) = \\frac{P(\\text{gang} = g, \\text{LOS} = 1)}{P(\\text{LOS} = 1)},\\] which is exactly row 1 divided by 0.25. Here’s a plot of this distribution. For comparison, we’ve also reproduced its marginal distribution. Interpretation: given information, about length of stay, we get an updated picture of the distribution of gang requests. Useful for decision making! 4.3.5 Law of Total Probability/Expectation Quite often, we know the conditional distributions, but don’t directly have the marginals. In fact, most of regression and machine learning is about seeking conditional means! (More in DSCI 561/571 +): For example, suppose you have the following conditional means of gang request given the length of stay of a ship. This curve is called a model function, and is useful if we want to predict a ship’s daily gang request if we know their length of stay. But what if we don’t know their length of stay, and we want to produce an expected gang request? We can use the marginal mean of gang request! In general, a marginal mean can be computed from the conditional means and the probabilities of the conditioning variable. The formula, known as the law of total expectation, is \\[E(Y) = \\sum_x E(Y \\mid X = x) P(X = x).\\] Here’s a table that outlines the relevant values: Length of Stay (LOS) E(Gang | LOS) P(LOS) 1 3.140580 0.25 2 2.412802 0.35 3 1.917192 0.20 4 1.596041 0.10 5 1.273317 0.10 Multiplying the last two columns together, and summing, gives us the marginal expectation: 2.3. Also, remember that probabilities are just means, so the result extends to probabilities: \\[P(Y = y) = \\sum_x P(Y = y \\mid X = x) P(X = x)\\] This is actually a generalization of the law of total probability we saw before: \\(P(Y=y)=\\sum_x P(Y = y, X = x)\\). 4.3.6 Exercises (10 min) In pairs, come to a consensus with the following three questions. Given the conditional means of gang requests, and the marginal probabilities of LOS in the above table, what’s the expected gang requests, given that the ship captain says they won’t be at port any longer than 2 days? In symbols, \\[E(\\text{Gang} \\mid \\text{LOS} \\leq 2).\\] What’s the probability that a new ship’s total gang demand equals 4? In symbols, \\[P(\\text{Gang} \\times \\text{LOS} = 4).\\] What’s the probability that a new ship’s total gang demand equals 4, given that the ship won’t stay any longer than 2 days? In symbols, \\[P(\\text{Gang} \\times \\text{LOS} = 4 \\mid \\text{LOS} \\leq 2).\\] 4.4 Dependence concepts A big part of data science is about harvesting the relationship between \\(X\\) and \\(Y\\), often called the dependence between \\(X\\) and \\(Y\\). 4.4.1 Independence (5 min) Informally, \\(X\\) and \\(Y\\) are independent if knowing something about one tells us nothing about the other. Formally, the definition of \\(X\\) and \\(Y\\) being independent is: \\[P(X = x \\cap Y = y) = P(X = x) P(Y = y).\\] A bunch of things follow as a consequence of independence. Here are some of them: \\(P(Y = y \\mid X = x) = P(Y = y)\\) (and likewise if you switch \\(X\\) and \\(Y\\)). \\(E(Y \\mid X = x) = E(Y)\\) \\(E[XY]=E[X]E[Y]\\) Notes: if \\(X\\) and \\(Y\\) are independent, we don’t actually need the whole table! All we need are the marginals of \\(X\\) and \\(Y\\), and we now have the full joint distribution between \\(X\\) and \\(Y\\) as just the product of the probabilities. independence isn’t some property that you can specify separately from the pmf. The pmf specifes everything about the situation, including whether or not the RVs are independent. 4.4.2 Measures of dependence (15 min) Next, if two random variables aren’t independent, how can we go about measuring the strength/amount of dependence between two random variables? 4.4.2.1 Covariance and Pearson’s Correlation Covariance is one common way of measuring dependence between two random variables. The idea is to take the average “signed area” of rectangles constructed between a sampled point and the mean, with the sign being determined by “concordance” relative to the mean: Concordant means \\(x &lt; \\mu_x\\) and \\(y &lt; \\mu_y\\), OR \\(x &gt; \\mu_x\\) and \\(y &gt; \\mu_y\\) – gets positive area. Discordant means \\(x &lt; \\mu_x\\) and \\(y &gt; \\mu_y\\), OR \\(x &gt; \\mu_x\\) and \\(y &lt; \\mu_y\\) – gets negative area. Here is a random sample of 10 points, with the 10 rectangles constructed with respect to the mean. Sign is indicated by colour. The covariance is the mean signed area. Formally, the definition is \\[\\mathrm{Cov(X, Y)} = E[(X-\\mu_X)(Y-\\mu_Y)],\\] where \\(\\mu_Y=E(Y)\\) and \\(\\mu_X=E(X)\\). This reduces to a more convenient form, \\[\\text{Cov}(X,Y)=E(XY)-E(X)E(Y)\\] In R, you can calculate the empirical covariance using the cov function: cov(j_sample$los, j_sample$gang) ## [1] -0.7111111 In the above example, the boxes are more often negative, so the covariance (and the “direction of dependence”) is negative. For the above example, the larger the LOS, the smaller the gang demand – this inverse relationship is indicative of negative covariance. Other interpretations of the sign: Positive covariance indicates that an increase in one variable is associated with an increase in the other variable. Zero covariance indicates that there is no linear trend – but this does not necessarily mean that \\(X\\) and \\(Y\\) are independent! It turns out covariance by itself isn’t very interpretable, because it depends on the scale (actually, spread) of \\(X\\) and \\(Y\\). For example, multiply \\(X\\) by 10, and suddenly the box sizes increase by a factor of 10, too, influencing the covariance. Pearson’s correlation fixes the scale problem by standardizing the distances according to standard deviations \\(\\sigma_X\\) and \\(\\sigma_Y\\), defined as \\[\\text{Corr}(X, Y) = E\\left[ \\left(\\frac{X-\\mu_X}{\\sigma_X}\\right) \\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right) \\right] =\\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}.\\] As a result, it turns out that \\[-1 \\leq \\text{Corr}(X, Y) \\leq 1.\\] The Pearson’s correlation measures the strength of linear dependence: -1 means perfect negative linear relationship between \\(X\\) and \\(Y\\). 0 means no linear relationship (Note: this does not mean independent!) 1 means perfect positive linear relationship. In R, you can calculate the empirical Pearson’s correlation using the cor function: cor(j_sample$los, j_sample$gang) ## [1] -0.6270894 Pearson’s correlation is ubiquitous, and is often what is meant when “correlation” is referred to. 4.4.2.2 Kendall’s tau Although Pearson’s correlation is ubiquitous, its forced adherance to measuring linear dependence is a big downfall, especially because many relationships between real world variables are not linear. An improvement is Kendall’s tau (\\(\\tau_K\\)): Instead of measuring concordance between each observation \\((x, y)\\) and the mean \\((\\mu_x, \\mu_y)\\), it measures concordance between each pair of observation \\((x_i, y_i)\\) and \\((x_j, y_j)\\). Instead of averaging the area of the boxes, it averages the amount of concordance and discordance by taking the difference between number of concordant and number of discordant pairs. Visually plotting the \\(10 \\choose 2\\) boxes for the above sample from the previous section: The formal definition is \\[\\frac{\\text{Number of concordant pairs} - \\text{Number of discordant pairs}}{{n \\choose 2}},\\] with the “true” Kendall’s tau value obtainined by sending \\(n \\rightarrow \\infty\\). Note that several ways have been proposed for dealing with ties, but this doesn’t matter when we’re dealing with continuous variables (Weeks 3 and 4). In R, the empirical version can be calculated using the cor() function with method = &quot;kendall&quot;: cor(j_sample$los, j_sample$gang, method = &quot;kendall&quot;) ## [1] -0.579771 Like Pearson’s correlation, Kendall’s tau is also between -1 and 1, and also measures strength (and direction) of dependence. For example, consider the two correlation measures for the following data set. Note that the empirical Pearson’s correlation for the following data is not 1! Pearson Kendall 0.9013 1 But, Kendall’s tau still only measures the strength of monotonic dependence. This means that patterns like a parabola, which are not monotonically increasing or decreasing, will not be captured by Kendall’s tau either: Pearson Kendall 0 0 Even though both dependence measures are 0, there’s actually deterministic dependence here (\\(X\\) determines \\(Y\\)). But, luckily, there are many monotonic relationships in practice, making Kendall’s tau a very useful measure of dependence. 4.4.3 Variance of a sum (2 min) Something you ought to know:\\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y).\\] This means that \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are independent. Another thing: if \\(X\\) and \\(Y\\) are independent, then \\(E(XY) = E(X)E(Y)\\). END OF QUIZ 1 MATERIAL 4.4.4 Dependence as separate from the marginals (5 min) (Optional) The amount of monotonic dependence in a joint distribution, as measured by kendall’s tau, has nothing to do with the marginal distributions. To demonstrate, here are joint distributions between LOS and gang demand having the same marginals, but different amounts of dependence. 4.4.5 Dependence as giving us more information (5 min) (Optional) Let’s return to the computation of the conditional distribution of gang requests given that a ship will only stay at port for one day. Let’s compare the marginal distribution (the case where we know nothing) to the conditional distributions for different levels of dependence (like we saw in the previous section). The means for each distribution are indicated as a vertical line: What’s of particular importance is comparing the uncertainty in these distributions. Let’s look at how the uncertainty measurements compare between marginal and conditional distributions (marginal measurements indicated as horizontal line): Moral of the story: more dependence (in either direction) gives us more certainty in the conditional distributions! This makes intuitive sense, because the more related \\(X\\) and \\(Y\\) are, the more that knowing what \\(X\\) is will inform what \\(Y\\) is. "],
["continuous-distributions.html", "Lecture 5 Continuous Distributions 5.1 Roadmap 5.2 Learning Objectives 5.3 Continuous random variables (10 min) 5.4 Density Functions (20 min) 5.5 Distribution Properties (25 min)", " Lecture 5 Continuous Distributions 5.1 Roadmap The first two weeks covered the following concepts: Distributions, and ways of describing a distribution. Simulation concepts. Bivariate concepts. All of this was in the context of discrete random variables. For the next two weeks, we will broaden our discussion on these topics, but in the context of continuous random variables. 5.2 Learning Objectives From today’s lecture, students should be able to: Differentiate between continuous and discrete random variables. Calculate and interpret probabilistic quantities (mean, quantiles, prediction intervals, etc) for a continuous random variable Match a distribution to sampled data A note on integrals: Working with continuous random variables sometimes involves integrating some functions. The most we will ever ask you to integrate are functions that are linear, because they can be computed using the formula for the area of a triangle and area of a rectangle. This is not because it’s “too hard” to compute integrals, but because there’s not much pedagogical value in it, especially because we’re gearing this course towards the usefulness of probability in data science. 5.3 Continuous random variables (10 min) What is the current water level of the Bow River at Banff, Alberta? How tall is a tree? What about the current atmospheric temperature in Vancouver, BC? These are examples of continuous random variables, because there are an uncountably infinite amount of outcomes. Discrete random variables, on the other hand, are countable, even if there are infinitely many outcomes, because each outcome can be accounted for one-at-a-time by some pattern. Example: The positive integers are discrete/countable: just start with 1 and keep adding 1 to get 1, 2, 3, etc., and that covers all possible outcomes. Positive real numbers are not countable because there’s no way to cover all possibilities by considering one outcome at a time. It turns out that it’s trickier to interpret probabilities for continuous random variables, but it also turns out that they’re in general easier to work with. Not all random variables with infinitely many outcomes are continuous. Take, for example, a Poisson random variable, that can take values \\(0, 1, 2, \\ldots\\) with no upper limit. The difference here is that a smaller range of values does have a finite amount of variables. By the way, this type of infinity is called “countably infinite”, and a continuous random variable has “uncountably infinite” amount of outcomes. In practice, we can never measure anything on a continuous scale, since any measuring instrument must always round to some precision. For example, your kitchen scale might only measure things to the nearest gram. But, these variables are well approximated by a continuous variable. As a rule of thumb, if the difference between neighbouring values isn’t a big deal, consider the variable continuous. Example: You’d like to get a handle on your monthly finances, so you record your total monthly expenses each month. You end up with 20 months worth of data: ## [1] &quot;$1903.68&quot; &quot;$3269.61&quot; &quot;$6594.05&quot; &quot;$1693.94&quot; &quot;$2863.71&quot; &quot;$3185.01&quot; ## [7] &quot;$4247.04&quot; &quot;$2644.27&quot; &quot;$8040.42&quot; &quot;$2781.11&quot; &quot;$3673.23&quot; &quot;$4870.13&quot; ## [13] &quot;$2449.53&quot; &quot;$1772.53&quot; &quot;$7267.11&quot; &quot;$938.67&quot; &quot;$4625.33&quot; &quot;$3034.81&quot; ## [19] &quot;$4946.4&quot; &quot;$3700.16&quot; Since a difference of $0.01 isn’t a big deal, we may as well treat this as a continuous random variable. Example: Back in the day when Canada had pennies, you liked to play “penny bingo”, and wrote down your winnings after each day of playing the game with your friends. Here are your net winnings: ## [1] 0.01 -0.01 0.02 0.01 0.04 0.02 -0.03 -0.01 0.05 0.04 Since a difference of $0.01 is a big deal, best to treat this as discrete. 5.4 Density Functions (20 min) In the discrete case, we were able to specify a distribution by indicating a probability for each outcome. Even when there’s an infinite amount of outcomes, such as in the case of a Poisson distribution, we can still place a non-zero probability on each outcome and have the probabilities sum to 1 (thanks to convergent series). But an uncountable amount of outcomes cannot be all accounted for by a sum (i.e., the type of sum we denote by \\(\\sum\\)), and this means that continuous outcomes must have probability 0. Example: The probability that the temperature in Vancouver tomorrow will be 18 degrees celcius is 0. In fact, any temperature has a probability of 0 of occurring. While individual outcomes have zero probability, ranges can have non-zero probability. We can use this idea to figure out how “dense” the probability is at some areas of the outcome space. For example, if a randomly selected tree has a 0.05 probability of being within 0.1m of 5.0m, then as a rate, that’s about 0.05/(0.1m) = 0.5 “probability per meter” here. Taking the limit as the range width \\(\\rightarrow 0\\), we obtain what’s called the density at 5m. The density as a function over the outcome space is called the probability density function (pdf), usually abbreviated to just the density, and denoted \\(f\\). Sometimes we specify the random variable in the subscript, just to be clear about what random variable this density represents – for example, \\(f_X\\) is the density of random variable \\(X\\). You’ll see that the density is like a “continuous cousin” of the probability mass function (pmf) in the case of discrete random variables. We’ll also see in a future lecture that there are some random variables for which neither a density nor a pmf exist. We can use the density to calculate probabilies of a range by integrating the density over that range: \\[P(a &lt; X &lt; b) = \\int_a^b f(x) \\text{d}x.\\] This means that, integrating over the entire range of possibilities should give us 1: \\[\\int_{-\\infty}^\\infty f(x) \\text{d}x = 1\\] This integral corresponds to the entire area under the density function. 5.4.1 Example: “Low Purity Octane” You just ran out of gas, but luckily, right in front of a gas station! Or maybe not so lucky, since the gas station is called “Low Purity Octane”. They tell you that the octane purity of their gasoline is random, and has the following density: What’s the probability of getting 25% purity? That is, \\(P(\\text{Purity} = 0.25)\\)? The density evaluates to be &gt;1 in some places. Does this mean that this is not a valid density? Why is the density in fact valid? Is it possible for the density to be negative? Why or why not? What’s the probability of getting gas that’s \\(&lt;50\\%\\) pure? That is, \\(P(\\text{Purity} &lt; 0.5)\\)? What’s the probability of getting gas that’s \\(\\leq 50\\%\\) pure? That is, \\(P(\\text{Purity} \\leq 0.5)\\)? What’s the support of this random variable? That is, the set of all outcomes that have non-zero density? You decide to spend the day at Low Purity Octane, measuring the octane purity for each customer. You end up getting 100 observations, placing each measurement along the x-axis. Which of the following plots would be more likely, and why? 5.4.2 Example: Monthly Expenses It turns out your monthly expenses have the following density, with your 20 observations plotted below it: 5.5 Distribution Properties (25 min) With continuous random variables, it becomes easier to expand our “toolkit” of the way we describe a distribution / random variable. As before, each property always has a distribution-based definition that gives us an exact/true value, and sometimes has an empirically-based (data-based) definition that gives us an approximate value, but that approaches the true value as more and more observations are collected. 5.5.1 Mean, Variance, Mode, and Entropy (again) (5 min) These are the properties of a distribution that we’ve already seen, but they do indeed extend to the continuous case. Mode and entropy can be defined, but since these ignore the numeric property of continuous random variables, they tend to not be used. Also, these properties don’t really have a natural empirical version. Mode: The outcome having the highest density. That is, \\[\\text{Mode} = {\\arg \\max}_x f(x).\\] Entropy: The entropy can be defined by replacing the sum in the finite case with an integral: \\[\\text{Entropy} = \\int_x f(x) \\log f(x) \\text{d}x.\\] Instead, we prefer to describe a continuous random variable using properties that inform us about distances. The mean and variance are two such measures of central tendency and uncertainty, where the only difference with a continuous random variable is in the distribution-based definition, where the sum becomes an integral. Mean: The distribution-based definition is \\[E(X) = \\int_x x \\, f(x) \\text{d}x.\\] You may later learn that this is a point that is “as close as possible” to a randomly generated outcome, in the sense that its expected squared distance is as small as possible. Ends up being the “center of mass” of a probability density function, meaning that you could “balance” the density function on this single point without it “toppling over due to gravity”. Probably best interpreted as the long-run sample average (empirical mean). Variance: The distribution-based definition is \\[\\text{Var}(X) = E \\left( (X - \\mu_X)^2 \\right) = \\int_x (x - \\mu_X) ^ 2 \\, f(x) \\text{d}x,\\] where \\(\\mu_X = E(X)\\). While the mean minimizes the expected squared distance to a randomly generated outcome, this is the expected squared distance. Going back to the octane purity example from Low Purity Octane gas station: The mode is 1 (the highest purity possible!). The entropy works out to be \\[\\int_0^1 2x \\log(2x) \\text{d}x \\doteq 0.1931.\\] The mean ends up being not a very good purity (especially as compared to the mode!), and is \\[\\int_0^1 2x^2 \\text{d}x = \\frac{2}{3}.\\] The variance ends up being \\[\\int_0^1 2 x \\left(x - \\frac{2}{3}\\right)^2 \\text{d}x = \\frac{1}{18}.\\] 5.5.2 Median (5 min) The median is the outcome for which there’s a 50-50 chance of seeing a greater or lesser value. So, its distribution-based definition satisfies \\[P(X \\leq \\text{Median}(X)) = 0.5.\\] Its empirically-based definition is the “middle value” after sorting the outcomes from left-to-right. Similar to the mean, you may later learn that the median is a point that is “as close as possible” to a randomly generated outcome, in the sense that its expected absolute distance is as small as possible. The median is perhaps best for making a single decision about a random outcome. Making a decision is simplest when the possibilities are reduced down to two equally likely outcomes, and this is exactly what the median does. For example, if the median time it takes to complete a hike is 2 hours, then you know that there’s a 50-50 chance that the hike will take over 2 hours. If you’re instead told that the mean is 2 hours, this only tells us that the total amount of hiking time done by a bunch of people will be as if everyone takes 2 hours to do the hike – this is still useful for making a decision about whether or not you should do the hike, but is more convoluted. Using the purity example at Low Purity Octane, the median is about 0.7071: 5.5.3 Quantiles (5 min) More general than a median is a quantile. The definition of a \\(p\\)-quantile \\(Q(p)\\) is the outcome that has a \\(1-p\\) probability of exceedance, or equivalently, for which there’s a probability \\(p\\) of getting a smaller outcome. So, its distribution-based definition satisfies \\[P(X \\leq Q(p)) = p.\\] The median is a special case, and is the 0.5-quantile. An empirically-based definition of the \\(p\\)-quantile is the \\(np\\)’th largest (rounded up) observation in a sample of size \\(n\\). Some quantiles have a special name: The 0.25-, 0.5-, and 0.75-quantiles are called quartiles. Sometimes named the first, second, and third quartiles, respectively. The 0.01-, 0.02, …, and 0.99-quantiles are called percentiles. Sometimes the \\(p\\)-quantile will be called the \\(100p\\)’th percentile; for example, the 40th percentile is the 0.4-quantile. Less commonly, there are even deciles, as the 0.1, 0.2, …, and 0.9-quantiles. For example, the 0.25-quantile of octane purity at Low Purity Octane is 0.5, since the area to the left of 0.5 is 0.25: 5.5.4 Prediction Intervals (5 min) It’s often useful to communicate an interval for which a random outcome will fall in with a pre-specified probability \\(p\\). Such an interval is called a \\(p \\times 100\\%\\) Prediction Interval. Usually, we set this up in such a way that there’s a \\(p/2\\) chance of exceeding the interval, and \\(p/2\\) chance of undershooting the interval. You can calculate the lower limit of this interval as the \\((1 - p)/2\\)-Quantile, and the upper limit as the \\(1 - (1 - p)/2\\)-Quantile. Example: a 90% prediction interval for the purity of gasoline at “Low Purity Octane” is [0.2236, 0.9746], composed of the 0.05- and 0.95-quantiles. 5.5.5 Skewness (5 min) Skewness measures how “lopsided” a distribution is, as well as the direction of the skew. If the density is symmetric about a point, then the skewness is 0. If the density is more “spread-out” towards the right / positive values, then the distribution is said to be right-skewed (positive skewness). If the density is more “spread-out” towards the left / negative values, then the distribution is said to be left-skewed (negative skewness). It turns out that for symmetric distributions, the mean and median are equivalent. But otherwise, the mean tends to be further into the skewed part of the distribution. Using the monthly expense example, the mean monthly expense is $3377.87, which is bigger than the median monthly expense of $2980.96. Formally, skewness can be defined as \\[\\text{Skewness} = E \\left( \\left( \\frac{X - \\mu_X}{\\sigma_X} \\right) ^ 3 \\right),\\] where \\(\\mu_X = E(X)\\) and \\(\\sigma_X = \\text{SD}(X)\\). For example, the octane purity distribution is left-skewed, and has a skewness of \\[\\int_0^1 2 x \\left(\\sqrt{18} (x - 2/3) \\right) ^ 3 \\text{d}x \\doteq -0.5657.\\] 5.5.6 Examples For the following situations, which quantity is most appropriate, and why? You want to know your monthly expenses in the long run (say, for forecasting net gains after many months). How do you communicate total expense? You want to ensure you put enough money aside on a given month to ensure you’ll have enough money to pay your bills. How much should you put aside? How should you communicate the cost of a typical house in North Vancouver? "],
["joint-probability-part-ii.html", "Lecture 6 Joint Probability, Part II 6.1 Learning Objectives 6.2 Depicting Distributions (25 min) 6.3 Common Distribution Families: Continuous, Part I (15 min) 6.4 Multivariate Distributions: Continuous (20 min) 6.5 Conditional Distributions, revisited (15 min)", " Lecture 6 Joint Probability, Part II Today, we’ll continue our discussion on continuous distributions, then move on to bivariate continuous distributions. 6.1 Learning Objectives From today’s class, students are expected to be able to: Explain whether a function is a valid pdf, cdf, qf, sf. Calculate mean and quantiles from a cdf, survival function, or quantile function. Use the R functions for distribution families to compute distributional quantities or generate a random sample. Specifically the functions of the form &lt;x&gt;&lt;dist&gt;, such as rnorm() or qunif(). Identify what makes a function a bivariate density/pdf. Compute conditional distributions when events have zero probability. 6.2 Depicting Distributions (25 min) So far, we’ve been saying that a pmf or a pdf is a distribution. Actually, there are more ways we can depict a distribution aside from the pmf/pdf. This section takes a deeper dive into alternative ways a probability distribution can be depicted, and their usefulness. Keep in mind that all of these depictions capture everything about a distribution, which means that if one of them is given, then the other ones can be derived. 6.2.1 Cumulative Density Functions (cdf’s) / Distribution Functions The cdf is usually denoted by \\(F\\), and is defined as \\[F(x) = P(X \\leq x).\\] We can calculate this using a density \\(f\\) by \\[F(x) = \\int_{-\\infty}^x f(t) \\, \\text{d}t.\\] Unlike the pdf/pmf, the cdf always exists for any random variable. It just doesn’t exist for categorical variables, because there’s no such thing as “less than” or “greater than”. For discrete random variables, the cdf is still a continuous function, but has a jump-discontinuity at the discrete values. Here are the cdf’s of the octane purity, monthly expenses, and length of stay (from last time): For the discrete cdf, a hollow point is a limiting point – the cdf does not evaluate to that point. Note that usually jump discontinuities in a cdf are connected with a straight vertical line, which we will do from now on after this plot. In order for a function \\(F\\) to be a valid cdf, the function needs to satisfy the following requirements: Must never decrease. It must never evalute to be &lt;0 or &gt;1. \\(F(x) \\rightarrow 0\\) as \\(x \\rightarrow -\\infty\\) \\(F(x) \\rightarrow 1\\) as \\(x \\rightarrow \\infty\\). The empirical cdf (ecdf) for a sample of size \\(n\\) treats the sample as if they are discrete values, each with probability \\(1/n\\). Like the cdf of a discrete random variable, the ecdf is also a “step function”. Here is the empirical cdf for the sample of 20 monthly expenses: 6.2.1.1 Exercise (10 min) On the board, let’s calculate the cdf’s of the following two distributions (that you’ve seen in lab): \\[X \\sim \\text{Discrete Uniform}(0, 4)\\] \\[Y \\sim \\text{Continuous Uniform}(0, 4)\\] 6.2.1.2 Evaluating Properties using the cdf (5 min) It turns out that the mean can be calculated in a fairly simple way from the cdf. It’s the area above the cdf and to the right of \\(x = 0\\), minus the area below the cdf and to the left of \\(x = 0\\). In-class exercise: the cdf of octane purity is \\[ F_{\\text{Purity}}(x) = \\begin{cases} 0, \\: x &lt; 0\\\\ x^2, \\: 0 \\leq x \\leq 1, \\\\ 1, \\: x &gt; 1. \\end{cases} \\] What is \\(P(0.5 &lt; \\text{Octane} &lt; 0.75)\\)? What is \\(P(0.5 &lt; \\text{Octane} \\leq 0.75)\\)? What is \\(P(\\text{Octane} &gt; 0.75)\\)? What is the median? 0.25-quantile? True or False: knowing the density of a distribution means that we also know the cdf; but knowing the cdf does not imply knowing the density. 6.2.2 Survival Function (2 min) The survival function \\(S\\) is just the cdf “flipped upside down”. For random variable \\(X\\), the survival function is defined as \\[S(x) = P(X &gt; x) = 1 - F(x).\\] The name comes from Survival Analysis (covered in DSCI 562), where \\(X\\) is interpreted as a “time of death”, so that the survival function is the probability of surviving beyond \\(x\\). Aside from Survival Analysis, the survival function is also useful for Extreme Value Theory. Here are the survival functions of our three examples: 6.2.3 Quantile Function (5 min) The quantile function \\(Q\\) takes a probability \\(p\\) and maps it to the \\(p\\)-quantile. It turns out that this is the inverse of the cdf! \\[Q(p) = F^{-1}(p)\\] Note that this function does not exist outside of \\(0 \\leq p \\leq 1\\)! This is unlike the other functions (density, cdf, and survival function), which exist on all real numbers. Here are the quantile functions of the examples we are working with: 6.2.4 Other ways of depicting a distribution (Optional) (1 min) There are even more ways to depict a distribution that we won’t be going into, that you might have heard of. Denote \\(X\\) as a random variable. Some are: Moment generating function (useful in mathematical statistics): \\[M(t) = E(e^{Xt})\\] Characteristic function (useful in mathematical statistics): \\[\\chi(t) = E(e^{Xti}),\\] where \\(i^2=1\\). Hazard function (useful in survival analysis; wait for DSCI 562): \\[h(t) = \\frac{f(t)}{S(t)}\\] 6.3 Common Distribution Families: Continuous, Part I (15 min) Just like for discrete distributions, there are also parametric families of continuous distributions. Again, I think Wikipedia is a great resource for looking up formulas and information about these families. We’re going to start with two common families for now. 6.3.1 Uniform (3 min) A Uniform distribution has equal density in between two points \\(a\\) and \\(b\\) (for \\(a &lt; b\\)), and is usually denoted by \\[\\text{Unif}(a, b).\\] That means that there are two parameters: one for each end-point. Reference to a “Uniform distribution” usually implies continuous uniform, as opposed to discrete uniform. The density is \\[f(x) = \\frac{1}{b - a} \\text{ for } a \\leq x \\leq b.\\] Here are some densities from members of this family: 6.3.2 Gaussian / Normal (4 min) Probably the most famous family of distributions. Has a density that follows a “bell-shaped” curve. Is usually parameterized by its mean \\(\\mu\\) and variance \\(\\sigma^2\\) (or sometimes just the standard deviation). A Normal distribution is usually denoted as \\[N(\\mu, \\sigma^2).\\] The density is \\[f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\\] Here are some densities from members of this family: 6.3.3 Log-Normal Family A random variable \\(X\\) as a Log-Normal distribution if \\(\\log X\\) is Normal. This family is often parameterized by the mean \\(\\mu\\) and variance \\(\\sigma^2\\) of \\(\\log X\\). The Log-Normal family is sometimes denoted, and this course will denote this family, as \\[LN(\\mu, \\sigma^2).\\] Here are some densities from members of this family: 6.3.4 Exponential Family The exponential family is for positive random variables, often interpreted as “wait time” for some event to happen. Characterized by a “memoryless” property, where after waiting for a certain period of time, the remaining wait time has the same distribution. The family is characterized by a single parameter, usually either the mean wait time, or its reciprocal, the average rate at which events happen. The densities from this family all decay starting at \\(x=0\\) for rate \\(\\beta\\): 6.3.5 Weibull Family A generalization of the Exponential family, which allows for an event to be more or less likely the longer you wait. Because of this flexibility and interpretation, this family is used heavily in survival analysis when modelling “time until an event”. This family is characterized by two parameters, a scale parameter \\(\\lambda\\) and a shape parameter \\(k\\) (where \\(k=1\\) results in the Exponential family). Here are some densities: 6.3.6 Beta Family The Beta family of distributions is defined for random variables taking values between 0 and 1, so is useful for modelling the distribution of proportions. This family is quite flexible, and has the Uniform distribution as a special case. Characterized by two positive shape parameters, \\(\\alpha\\) and \\(\\beta\\). Examples of densities: 6.3.7 Relevant R functions (8 min) R has functions for many distribution families. We’ve seen a few already in the case of discrete families, but here’s a more complete overview. The functions are of the form &lt;x&gt;&lt;dist&gt;, where &lt;dist&gt; is an abbreviation of a distribution family, and &lt;x&gt; is one of d, p, q, or r, depending on exactly what about the distribution you’d like to calculate. Possible prefixes &lt;x&gt;: d: density function - we call this \\(p\\) p: cumulative distribution function (cdf) - we call this \\(F\\) q: quantile function (inverse cdf) r: random sample generation Some abbreviations &lt;dist&gt;: unif: Uniform (continuous) norm: Normal (continuous) lnorm: Log-Normal (continuous) geom: Geometric (discrete) pois: Poisson (discrete) binom: Binomial (discrete) etc. Examples: The uniform family: dunif(), punif(), qunif(), runif() The Gaussian family: dnorm(), pnorm(), qnorm(), rnorm() Demonstration: What’s the density of the N(2, 4) distribution evaluated at the point \\(x = 3\\)? What’s the cdf of the Unif(0, 1) distribution evaluated at the points \\(x = 0.25, 0.5, 0.75\\)? What’s the median of the Unif(0, 2) distribution? Generate a random sample from the N(0, 1) distribution of size 10. 6.4 Multivariate Distributions: Continuous (20 min) In the discrete case we already saw joint distributions, conditional distributions, marginal distributions, etc. All that stuff carries over. Let’s start with two variables (“bivariate”). A note on depictions of distributions: There is such thing as a multivariate cdf. It comes in handy in copula theory, which is an optional question in a lab assignment. But otherwise, it’s not as useful as a multivariate density, so we won’t cover it. And, there’s no such thing as a multivariate quantile function. 6.4.1 Multivariate Densities/pdf’s Recall the joint pmf (discrete) from Lecture 4, between gang demand and length-of-stay: Gangs = 1 Gangs = 2 Gangs = 3 Gangs = 4 LOS = 1 0.0017 0.0425 0.1247 0.0811 LOS = 2 0.0266 0.1698 0.1360 0.0176 LOS = 3 0.0511 0.1156 0.0320 0.0013 LOS = 4 0.0465 0.0474 0.0059 0.0001 LOS = 5 0.0740 0.0246 0.0014 0.0000 Each entry in the table corresponds to the probability of that unique row (LOS value) and column (Gang value). These probabilities add to 1. For the continuous case, instead of rows and columns, we have an x- and y-axis for our two variables, defining a region of possible values. For example, if two marathon runners can only finish a marathon between 5.0 and 5.5 hours each, and their end times are totally random, then the possible values are indicated by a square in the following plot: Each point in the square is like an entry in the joint pmf table in the discrete case, except now instead of holding a probability, it holds a density. The density function, then, is a surface overtop of this square (or in general, the outcome space). That is, it’s a function that takes two variables (marathon time for Runner 1 and Runner 2), and calculates a single density value from those two points. This function is called a bivariate density function. Here’s an example of what a 2D pdf might look like: https://scipython.com/blog/visualizing-the-bivariate-gaussian-distribution/ Notation: For two random variables \\(X\\) and \\(Y\\), their joint density/pdf evaluated at the points \\(x\\) and \\(y\\) is usually denoted \\[f_{X,Y}(x,y),\\] or sometimes less rigorously, as just \\[f(x, y).\\] 6.4.1.1 Mike’s take on notation There’s a lot of sloppy notation used here. For example I’ll sometimes write \\(f(x\\mid y=0)\\) or \\(f(x\\mid Y=0)\\) which are probably better written as \\(f_{X\\mid Y=0}(x)\\), but it gets tiresome to write that out all the time. You may also see me write things like \\(f(x,y=0)\\) instead of \\(f_{X,Y}(x,0)\\); again, this is just shorthand. In the best possible world, the subscripts of \\(f\\) specify which function you’re actually talking about, and then the arguments are just entered in like any other mathematical function. But it’s often more convenient to just write \\(f\\) and let the reader figure out from context which function it is, exactly. I certainly wouldn’t want to see \\(f(3)\\) for a case with two random variables, though, as in that case it’s totally unclear if that’s one of the marginals (\\(f_X(3)\\) or \\(f_Y(3)\\)) or who knows what. But \\(f(2,3)\\) is sort of reasonable - that’s probably the joint density evaluated at \\(x=2,y=3\\), i.e. \\(f_{X,Y}(2,3)\\). 6.4.2 Calculating Probabilities Remember in the univariate continuous case, we calculated probabilities as the area under the density curve. In the bivariate case, since we have a density surface, we can calculate probabilities as the volume under the density surface. This means that the total volume under the density function must equal 1. Formally, this may be written as \\[\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f(x,y)dxdy=1\\] Example: If the density is equal/flat across the entire outcome space, what’s the height of this surface? That is, what does the density evaluate to? What does it evaluate to outside of the outcome space? What’s the probability that Runner 1 will finish the marathon before Runner 2? To calculate this, first identify the region in the outcome space that corresponds to this event. This is plotted below, as the darker triangle. Then, calculate the volume of the space overtop of this region and below the density surface. What’s the probability that Runner 1 finishes in 5.2 hours? Hint: this region is plotted below, indicated in a darker shade. 6.5 Conditional Distributions, revisited (15 min) Remember the formula for conditional probabilities: for events \\(A\\) and \\(B\\), \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\\] But, this is only true if \\(P(B) \\neq 0\\), and it’s not useful if \\(P(A) = 0\\) – two situations we’re faced with in the continuous world! 6.5.1 When \\(P(A) = 0\\) To describe this situation, let’s use a univariate continuous example: the example of monthly expenses. Suppose the month is half-way over, and you find that you only have $2500 worth of expenses so far! What’s the distribution of this month’s total expenditures now, given this information? If we use the law of conditional probability, we would get a formula that’s not useful: letting \\(X = \\text{Expense}\\), \\[P(X = x \\mid X \\geq 2500) = \\frac{P(X = x)}{P(X \\geq 2500)} \\ \\ \\ \\text{(no!)}\\] This is no good, because the outcome \\(x\\) has a probability of 0. This equation just simplies to 0 = 0, which is not useful. Instead, in general, we replace probabilities with densities. In this case, what we actually have is: \\[f(x \\mid X \\geq 2500) = \\frac{f(x)}{P(X \\geq 2500)} \\ \\text{ for } x \\geq 2500,\\] and \\(f(x \\mid X \\geq 2500) = 0\\) for \\(x &lt; 2500\\). Notice from the formula that the resulting density is just the original density confined to \\(x \\geq 2500\\), and re-normalized to have area 1. This is what we did in the discrete case! The monthly expense example has expenditures \\(X \\sim\\) LN(8, 0.5). Here is its marginal distribution and the conditional distribution. Notice the conditional distribution is just a segment of the marginal, and then re-normalized to have area 1. 6.5.2 When \\(P(B) = 0\\) To describe this situation, let’s use the marathon runners’ example again. Runner 1 ended up finishing in 5.2 hours. What’s the distribution of Runner 2’s time? Letting \\(X\\) be the time for Runner 1, and \\(Y\\) for Runner 2, we’re asking for \\(f_{Y|X}(y \\mid X = 5.2)\\). But wait! Didn’t we say earlier that \\(P(X = 5.2) = 0\\)? This is the bizarre nature of continuous random variables. Although no outcome is possible, we must observe some outcome in the end. In this case, the stopwatch used to calculate run time has rounded the true run time to 5.2h, even though in reality, it would have been something like 5.2133843789373… hours. As before, plugging in the formula for conditional probabilities won’t work. But, as the case when \\(P(A) = 0\\), we can in general replace probabilities with densities. We end up with \\[f_{Y|X}(y \\mid 5.2) = \\frac{f_{Y,X}(y, 5.2)}{f_X(5.2)}.\\] This formula is true in general \\[f_{Y|X}(y \\mid x) = \\frac{f_{Y,X}(y, x)}{f_X(x)}.\\] In fact, this formula is even true for both pdf’s and pmf’s! We’ll see what this looks like visually next time. "],
["dependence.html", "Lecture 7 Dependence 7.1 Learning Objectives 7.2 Drawing multidimensional functions (5 min) 7.3 Independence Revisited (10 min) 7.4 Harvesting Dependence (20 min) 7.5 Marginal Distributions (20 min) 7.6 Multivariate Gaussian/Normal Family (20 min)", " Lecture 7 Dependence Today’s topic is dependence, and in particular its implications in one variable informing us about another. We’ll end off with the Multivariate Gaussian distribution. 7.1 Learning Objectives From today’s lecture, students are expected to be able to: Interpret a contour plot, especially as a bivariate density. Identify the relationship between two independent continuous random variables and their conditional distributions (given one of the two variables). Extract the information that a random variable \\(X\\) holds about \\(Y\\) by conditioning on \\(X\\) and producing model functions. Compute marginal distributions and means using joint distributions, and especially using conditional distributions. Identify the direction of dependence from a contour plot of a joint density function. A note on plotting: You will not be expected to make contour plots like you see in these lecture notes. That will be saved for next block in DSCI 531: Data Visualization I. 7.2 Drawing multidimensional functions (5 min) Drawing a \\(d\\)-dimensional function requires \\(d+1\\) dimensions, so we usually just draw 1-dimensional functions, and occasionally draw 2-dimensional functions, and almost never draw functions with \\(d&gt;2\\) inputs. A common way to draw 2-dimensional functions is to use contour plots, but you can also think of the output dimension as coming out of the page. Here are some examples of how to draw 2-dimensional functions: in python and in R. Here’s an example of a 3D drawing of a volcano (taken directly out of the example in the documentation of rgl::surface3d()): You must enable Javascript to view this page properly. And, a contour plot rendering: When it comes to bivariate density functions (pdf’s), it’s informative to plot the marginal densities on each axis. Here’s an example where marginals are Gaussian, and the joint distribution is also Gaussian (we’ll see what this means later today): Remember, the density function tells us how “densely packed” data coming from this distribution will be. Here’s the same plot, but with a sample of size 150 plotted overtop. The individual \\(x\\) and \\(y\\) coordinates are also plotted below their corresponding densities. Notice that points are more densely packed near the middle, where the density function is biggest. 7.2.1 A possible point of confusion: empirical contour plots The contour + simulated data plot is meant to show you the relationship between the joint density function and the density of data points. In practice, we don’t know the joint density function nor the marginals, so we use an empirical version instead. You’ll learn about these in DSCI 531. Here’s an example of a contour plot of an empirical joint density function, and empirical marginal densities: 7.3 Independence Revisited (10 min) 7.3.1 Definition in the Continuous Case Recall that independence of random variables \\(X\\) and \\(Y\\) means that knowledge about one variable tells us nothing about another variable. In the discrete case, this means that a joint probability distribution (when depicted as a table) has each row/column as a multiple of the others, because (by definition of independence) \\[P(X = x, Y = y) = P(X = x) P(Y = y).\\] In the continuous case, as usual, probabilities become densities. A definition of independence then becomes \\[f_{X,Y}(x, y) = f_Y(y) \\ f_X(x).\\] This means that, when slicing the joint density at various points along the x-axis (also for the y-axis), the resulting one-dimensional function will be the same, except for some multiplication factor. Perhaps a more intuitive definition of independence is when \\(f_{Y \\mid X}(y \\mid x) = f_Y(y)\\) for all possible \\(x\\) – that is, knowing \\(X\\) does not tell us anything about \\(Y\\). The same could be said about the reverse. To see why this definition is equivalent to the above definition, consider a more general version of the above formula, which holds regardless of whether have have independence (we’ve seen this formula before): \\[f_{X,Y}(x, y) = f_{Y \\mid X}(y \\mid x) \\ f_X(x).\\] Setting \\(f_{Y \\mid X}\\) equal to \\(f_Y\\) results in the original definition above. 7.3.2 Independence Visualized In general, just by looking at a contour plot of a bivariate density, it’s hard to tell whether this distribution is of two independent random variables. But, we can tell by looking at “slices” of the distribution. Here is an example of two independent random variables, where \\(X \\sim \\text{Exp}(1)\\) and \\(Y \\sim N(0,1)\\). We’re going to slice the density along the dotted lines: Here are the slices. In each case: The slice axis gets put on the x-axis, and The “z-axis” (the one coming “out of the page”) gets put on the y-axis. Again, looking above, it’s not that each vertical (or horizontal) slice is the same, but they are all the same when the slice is normalized. In other words, every slice has the same shape. What do we get when we normalize these slices so that the curves have an area of 1 underneath? We get the conditional distributions given the slice value, by definition. And, these conditional distributions will be the exact same (one for each axis \\(X\\) and \\(Y\\)), since the sliced densities only differ by a multiple anyway. What’s more, this common distribution is just the marginal. Mathematically, what we’re saying is \\[f_{Y \\mid X}(y \\mid x) = \\frac{f_{X,Y}(x,y)}{f_X(x)} = \\frac{f_{X}(x) f_{Y}(y)}{f_X(x)} = f_Y(y)\\] (and the same for \\(X \\mid Y\\)). Again, we’re back to the definition of independence! Here’s another example of an independent joint distribution: 7.3.3 Activity The following are different scenarios of a bivariate density being sliced along different values of \\(X\\), and plotting the resulting surface overtop of the slice. Which of the following are examples of independence between \\(X\\) and \\(Y\\)? 7.4 Harvesting Dependence (20 min) The opposite of independence is dependence: when knowing something about \\(X\\) tells us something about \\(Y\\) (or vice versa). Extracting this “signal” that \\(X\\) contains about \\(Y\\) is at the heart of supervised learning (regression and classification), covered in DSCI 571/561 and beyond. Usually, we reserve the letter \\(X\\) to be the variable that we know something about (usually an exact value), and \\(Y\\) to be the variable that we want to learn about. These variables go by many names – usually, \\(Y\\) is called the response variable, and \\(X\\) is sometimes called a feature, or explanatory variable, or predictor, etc. To extract the information that \\(X\\) holds about \\(Y\\), we simply use the conditional distribution of \\(Y\\) given what we know about \\(X\\). This is as opposed to just using the marginal distribution of \\(Y\\), which corresponds to the case where we don’t know anything about \\(X\\). Sometimes it’s enough to just communicate the resulting conditional distribution of \\(Y\\), but usually we reduce this down to some of the distributional properties that we saw earlier, like mean, median, or quantiles. We communicate uncertainty also using methods we saw earlier, like prediction intervals and standard deviation. Let’s look at an example. 7.4.1 Example: River Flow In the Rocky Mountains, snowmelt \\(X\\) is a major driver of river flow \\(Y\\). Suppose the joint density can be depicted as follows: Every day, a measurement of snowmelt is obtained. To predict the river flow, usually the conditional mean of river flow given snowmelt is used as a prediction, but median is also used. Here are the two quantities as a function of snow melt: These functions are called model functions, and there are a ton of methods out there to help us directly estimate these model functions without knowing the density. This is the topic of supervised learning – even advanced supervised learning methods like deep learning are just finding a model function like this (although, usually when there are more than one \\(X\\) variable). It’s also quite common to produce prediction intervals. Here is an example of an 80% prediction interval, using the 0.1- and 0.9-quantiles as the lower and upper limits: As a concrete example, consider the case where we know there’s been 1mm of snowmelt. To obtain the conditional distribution of flow (\\(Y\\)) given this information, we just “slice” the joint density at \\(x =\\) 1, and renormalize. Here is that density (which is now univariate!), compared with the marginal distribution of \\(Y\\) (representing the case where we know nothing about snowmelt, \\(X\\)): The following table presents some properties of these distributions: Quantity Marginal Conditional Mean 247.31 118.16 Median 150 74.03 80% PI [41.64, 540.33] [25.67, 236.33] Notice that we actually only need the conditional distribution of \\(Y\\) given \\(X=x\\) for each value of \\(x\\) to produce these plots! In practice, we usually just specify these conditional distributions. So, having the joint density is actually “overkill”. 7.4.2 Direction of Dependence Two variables can be dependent in a multitude of ways, but usually there’s an overall direction of dependence: Positively related random variables tend to increase together. That is, larger values of \\(X\\) are associated with larger values of \\(Y\\). Negatively related random variables have an inverse relationship. That is, larger values of \\(X\\) are associated with smaller values of \\(Y\\). We’ve already seen some measures of dependence in the discrete setting: covariance, correlation, and Kendall’s tau. These definitions carry over. It’s a little easier to visualize the definition of covariance as the signed sum of rectangular area: Correlation, remember, is also the signed sum of rectangles, but after converting \\(X\\) and \\(Y\\) to have variances of 1. Here are two positively correlated variables, because there is overall tendency of the contour lines to point up and to the right (or down and to the left): Here are two negatively correlated variables, because there is overall tendency for the contour lines to point down and to the right (or up and to the left): Another example of negative correlation. Although contour lines aren’t pointing in any one direction, there’s more density along a line that points down and to the right (or up and to the left) than there is any other direction. Here are two random variables that are dependent, yet have 0 correlation (both Pearson’s and Kendall’s) because the overall trend is flat (pointing left or right). You can think of this in terms of slicing: slicing at \\(x = -2\\) would result in a highly peaked distribution near \\(y = 0\\), whereas slicing at \\(x = 1\\) would result in a distribution with a much wider spread – these are not densities that are multiples of each other! Prediction intervals would get wider with larger \\(x\\). Note that the marginal distributions have nothing to do with the dependence between random variables. Here are some examples of joint distributions that all have the same marginals (\\(N(0,1)\\)), but different dependence structures and strengths of dependence: 7.5 Marginal Distributions (20 min) In the river flow example, we used snowmelt to inform river flow by communicating the conditional distribution of river flow given snowmelt. But, this requires knowledge of snowmelt! What if one day we are missing an observation on snowmelt? Then, the best we can do is communicate the marginal distribution of river flow. But how can we get at that distribution? Usually, aside from the data, we only have information about the conditional distributions. But this is enough to calculate the marginal distribution! 7.5.1 Marginal Distribution from Conditional We can use the law of total probability to calculate a marginal density. Recall that for discrete random variables \\(X\\) and \\(Y\\), we have \\[P(Y = y) = \\sum_x P(X = x, Y = y) = \\sum_x P(Y = y \\mid X = x) P(X = x).\\] The same thing applies in the continuous case, except probabilities become densities and sums become integrals (as usual in the continuous world): for continuous \\(X\\) and \\(Y\\), \\[f_Y(y) = \\int_x f_{X,Y}(x,y)\\ \\text{d}x = \\int_x f_{Y\\mid X}(y \\mid x)\\ f_X(x)\\ \\text{d}x.\\] Notice that this is just an average of the conditional densities! If we have the conditional densities and a sample of \\(X\\) values \\(x_1, \\ldots, x_n\\), then using the empirical approximation of the mean, we have \\[f_Y(y) \\approx \\frac{1}{n} \\sum_{i = 1}^n f_{Y\\mid X}(y \\mid x_i).\\] A similar result holds for the cdf. We have \\[F_Y(y) = \\int_x F_{Y \\mid X}(y \\mid x)\\ f_X(x) \\ \\text{d}x,\\] and empirically, \\[F_Y(y) \\approx \\frac{1}{n}\\sum_{i = 1}^n F_{Y\\mid X}(y \\mid x_i).\\] 7.5.2 Marginal Mean from Conditional Perhaps more practical is finding the marginal mean, which we can obtain using the law of total expectation (similar to the discrete case we saw in a previous lecture): \\[E(Y) = \\int_x m(x) \\ f_{X}(x) \\ \\text{d}x = E(m(X)),\\] where \\(m(x) = E(Y \\mid X = x)\\) (i.e., the model function or regression curve). When you fit a model using supervised learning, you usually end up with an estimate of \\(m(x)\\). From the above, we can calculate the marginal mean as the mean of \\(m(X)\\), which we can do empirically using a sample of \\(X\\) values \\(x_1, \\ldots, x_n\\). Using the empirical mean, we have \\[E(Y) \\approx \\frac{1}{n} \\sum_{i=1}^n m(x_i).\\] 7.5.3 Marginal Quantiles from Conditional Unfortunately, if you have the \\(p\\)-quantile of \\(Y\\) given \\(X = x\\), then there’s no convenient way of calculating the \\(p\\)-quantile of \\(Y\\) as an average. To obtain this marginal quantity, you would need to calculate \\(F_Y(y)\\) (as above), and then find the value of \\(y\\) such that \\(F_Y(y) = p\\). 7.5.4 Activity You’ve observed the following data of snowmelt and river flow: Snowmelt (mm) Flow (m^3/s) 1 140 3 150 3 155 2 159 3 170 From this, you’ve deciphered that the mean flow given snowmelt is \\[E(\\text{Flow} \\mid \\text{Snowmelt} = x) = 100 + 20x.\\] You also decipher that the conditional standard deviation is constant, and is: \\[SD(\\text{Flow} \\mid \\text{Snowmelt} = x) = 15\\ m^3/s\\] It also looks like the conditional distribution of river flow given snowmelt follows a Lognormal distribution. Part 1: A new reading of snowmelt came in, and it’s 4mm. Make a prediction of river flow. What distribution describes your current understanding of what the river flow will be? Part 2: Your snowmelt-recording device is broken, so you don’t know how much snowmelt there’s been. Make a prediction of river flow. What distribution describes your current understanding of what the river flow will be? Someone tells you that a 90% prediction interval is [70, 170]. What do we know about the median? 7.6 Multivariate Gaussian/Normal Family (20 min) Moved to Lecture 8 "],
["noteworthy-distribution-families.html", "Lecture 8 Noteworthy Distribution Families 8.1 Learning Objectives 8.2 More Univariate Distribution Families (5 min) 8.3 Multivariate Gaussian/Normal Family (20 min) 8.4 Break and Evaluations (8 min) 8.5 Mixture distributions (20 min) 8.6 Topics in the Appendix", " Lecture 8 Noteworthy Distribution Families 8.1 Learning Objectives From today’s class, students are expected to be able to: Identify the parameterization of the multivariate Gaussian/Normal family. Describe the joint density of a bivariate Gaussian distribution in terms of its contours. Identify whether a bivariate Gaussian distribution has dependence or independence. Compute marginal and conditional distributions from a bivariate Gaussian distribution, and compute the distribution of a linear combination of jointly Gaussian random variables. Calculate the density or cdf of a mixture distribution, given the class probabilities and class distributions. 8.2 More Univariate Distribution Families (5 min) I added the following univariate distribution families to Lecture 6. It just fit better there, along with the other families listed there. Let’s take a look at them. They are: Exponential Weibull Beta 8.3 Multivariate Gaussian/Normal Family (20 min) We’ve already seen the Gaussian/Normal family of univariate distributions. There’s also a multivariate family of Gaussian distributions. Members of this family need to have all Gaussian marginals, and their dependence has to be “Gaussian dependence”. If you’re interested, “Gaussian dependence” is obtained as a consequence of requiring that any linear combination of Gaussian random variables is also Gaussian. 8.3.1 Parameters To characterize the bivariate Gaussian family, we need the following parameters: the parameters of the two marginals (mean and variance for both \\(X\\) and \\(Y\\), sometimes denoted \\(\\mu_X, \\mu_Y, \\sigma^2_X, \\sigma^2_Y\\)), and the covariance between \\(X\\) and \\(Y\\), sometimes denoted \\(\\sigma_{XY}\\) (or, equivalently, the pearson correlation, sometimes denoted \\(\\rho\\)). That’s five parameters altogether, and only one of them (Pearson correlation or covariance) is needed to specify the dependence part. Using the parameters of a bivariate Gaussian distribution, we can construct two objects that are useful for computations: a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\Sigma\\), where \\[\\boldsymbol{\\mu}=\\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix},\\] and \\[\\Sigma = \\begin{pmatrix} \\sigma_X^2 &amp; \\sigma_{XY} \\\\ \\sigma_{XY} &amp; \\sigma_Y^2 \\end{pmatrix}.\\] Even though \\(\\sigma_{XY}\\) is repeated in the upper-right and lower-left corner of \\(\\Sigma\\), constructing the matrix in this way makes for much easier computations down the road. Note that the covariance matrix is always defined as above. Even if we’re given the correlation \\(\\rho\\) instead of the covariance \\(\\sigma_{XY}\\), we would then need to calculate the covariance (as \\(\\sigma_{XY} = \\rho \\sigma_X \\sigma_Y\\)) before constructing the covariance matrix. However, there is another matrix that is sometimes useful, called the correlation matrix, and it’s defined as \\[\\text{Correlation Matrix} = \\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix}.\\] This “linear algebra” format of the parameters also makes it easier to generalize to more than two variables. In general, the multivariate Gaussian distribution made up of \\(d\\) variables has some generic \\(d\\)-dimensional mean vector, and a \\(d \\times d\\) covariance matrix, where the upper-right triangle and lower-right triangle of the covariance matrix are the same. This means that, to fully specify this \\(d\\)-dimensional distribution, we need: the means and variances of all \\(d\\) random variables, and the covariance or correlations between each pair of random variables (that’s \\(d \\choose 2\\) of them). If you’re interested, it turns out any square matrix is a valid covariance matrix, so long as it’s positive definite. This takes care of the fact that the individual variances can’t be negative and \\(-1\\leq\\rho\\leq1\\), or put another (more confusing) way, \\(| Cov(X,Y) | \\leq \\sqrt{Var(X)Var(Y)}\\). If you’d like a brief review of linear algebra, check out the Appendix. Here’s an example of a covariance matrix with \\(d=3\\) (“trivariate”), and random variables \\(X, Y, Z\\): \\[\\Sigma=\\begin{pmatrix} \\sigma_X^2 &amp; \\sigma_{XY} &amp; \\sigma_{XZ} \\\\ \\sigma_{XY} &amp; \\sigma_Y^2 &amp; \\sigma_{YZ}\\\\ \\sigma_{XZ} &amp; \\sigma_{YZ} &amp; \\sigma_Z^2 \\end{pmatrix}\\] There are overall 9 parameters needed to characterize the trivariate Gaussian family: 6 for the marginals (mean and variance per marginal), and 3 dependence parameters (all pairwise correlations). 8.3.2 Visualizing Bivariate Gaussian Density The joint density of multivariate Gaussian distributions have a characteristic “elliptical” shape to them. Here are some examples with \\(N(0,1)\\) marginals, with different Pearson correlation amounts indicated in the bars: And here are samples of data coming from these distributions: Indeed, for Gaussians specifically, uncorrelated implies \\(X\\) and \\(Y\\) are independent. But, remember, uncorrelated often does not imply independence. Let’s take a look at uncorrelated densities, but with different variances, and means of 0: Notice that elliptical contours stretched either vertically or horizontally still have no dependence! None of these do. The stretch needs to be on some diagonal in order for there to be dependence – that is, pointing in some direction other than along the x-axis or y-axis. Circular contours are both independent and each marginal has the same variance. Note: you’ll notice the mean vector isn’t very interesting, it just shifts things around. The interesting stuff lives in \\(\\Sigma\\). Optional note: you’ll notice the contours are ellipses (ellipsoids in higher dimensions). You may recall from linear algebra class that a matrix (specifically its eigenvalues) can be thought to represent an ellipse/ellipsoid. This is the covariance matrix here – not a coincidence. 8.3.3 Properties This distribution has many amazing properties. Marginal distributions are Gaussian. The marginal distribution of a subset of variables can be obtained by just taking the relevant subset of means, and the relevant subset of the covariance matrix. Linear combinations are Gaussian. This is actually by definition. If \\((X, Y)\\) have a bivariate Gaussian distribution, then \\(aX + bY + c\\) for constants \\(a, b, c\\) is Gaussian. Want to find the mean and variance? Just apply the linearity of expectations and variance rules we saw earlier: \\[E(aX + bY + c) = a \\mu_X + b \\mu_Y + c,\\] and \\[\\text{Var}(aX + bY + c) = a^2 \\sigma_X^2 + b^2 \\sigma_Y^2 + 2ab\\sigma_{XY}.\\] The same rules apply with more than two Gaussian random variables. Conditional distributions are Gaussian. If \\((X, Y)\\) have a bivariate Gaussian distribution, then the distribution of \\(Y\\) given that \\(X = x\\) is also Gaussian. Its distribution is \\[Y\\mid X = x \\sim N \\left(\\mu_Y + \\frac{\\sigma_Y}{\\sigma_X}\\rho (x - \\mu_x),\\ (1 - \\rho^2)\\sigma_Y^2 \\right)\\] Take a moment to notice what’s going on here: The conditional mean is linear in \\(x\\), passes through the mean \\((\\mu_X, \\mu_Y)\\), and has a steeper slope with higher correlation. The conditional variance is smaller than the marginal variance, and gets smaller with higher correlation. Here are the conditional means (“regression line”) and 90% prediction intervals for the previous plots of bivariate Gaussians with different correlations. Note that the regression line does not actually pass through the ellipse from “tip to tip”, except in the independent case! If you want to know the formula for conditional distributions in the general multivariate case, you can find this pretty easily online (c.f. Wikipedia). It involves matrix algebra with the covariance matrix and mean vector. 8.3.4 Activity Consider the multivariate Gaussian distribution of random variables \\(X\\), \\(Y\\), and \\(Z\\) with (respective) mean vector \\[\\boldsymbol{\\mu} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 3 \\end{pmatrix},\\] correlation matrix \\[\\begin{pmatrix} 1 &amp; 0.2 &amp; 0.1 \\\\ 0.2 &amp; 1 &amp; 0.2 \\\\ 0.1 &amp; 0.2 &amp; 1 \\end{pmatrix},\\] and marginal variances of 1. What’s the distribution of \\(X\\)? What’s the joint distribution of \\(X\\) and \\(Z\\)? What’s the distribution of \\(Y\\), given that \\(X = 0.5\\)? What’s the distribution of \\(Y - 3X\\)? What’s \\(P(Y &lt; 3X)\\)? 8.4 Break and Evaluations (8 min) We’ll take a bit of a longer break so that you can fill in instructor evaluations. Please fill them out online (you should have received an email). 8.5 Mixture distributions (20 min) In Lecture 3 (Simulations), we used simulation to find the distribution of a random variable that involves multiple steps. We saw at least two examples of this: The total gang demand on a given day for ships arriving at port: Generate the number of ships arriving at port on a given day. For each ship, generate a gang demand. From your lab, the total number of cupcakes you’ll need at a party: Generate attendance to a party from a guestlist. For each person attending, generate the number of cupcakes eaten. There’s an important type of multiple-step distribution called a mixture distribution, that shows up in many applications. In general, a mixture distribution results if an outcome is generated as follows: Generate a “membership” into one of \\(k\\) possible classes \\(A_1, \\ldots, A_k\\), each having probabilities \\(p_1, \\ldots, p_k\\). Each class has its own distribution; generate an observation from the corresponding distribution. If the individual distributions in Step 2 have pdf’s/pmf’s \\(f_1, \\ldots, f_k\\), then it turns out the pdf/pmf \\(f_{\\text{Mixture}}\\) of an outcome generated by these two steps can be calculated by \\[f_{\\text{Mixture}}(x) = p_1 f_1(x) + \\cdots + p_k f_k(x).\\] This is true whether or not the pdf’s/pmf’s are univariate or multivariate! We also have a similar result for cdf’s. If \\(F_1, \\ldots, F_k\\) are the cdf’s corresponding to \\(f_1, \\ldots, f_k\\), then the cdf of the mixture distribution can be calculated by \\[F_{\\text{Mixture}}(x) = p_1 F_1(x) + \\cdots + p_k F_k(x).\\] As usual, this formula isn’t a “new rule” in probability – in fact, we can derive the above formula using the law of total probability. But, we won’t require that for this course. 8.5.1 Example: Mixture of Gaussians A common example of a mixture distribution is a mixture of Gaussians, where each of the distribution classes \\(f_1, \\ldots, f_k\\) are Gaussian distributions (whether univariate or multivariate). Univariate Example: Normally, the time it takes you to commute to work (in minutes) follows a \\(N(20, 16)\\) distribution. But, there’s a whopping 20% chance that there’ll be a collision along the way that will slow you down, in which case it will take you \\(N(40, 25)\\) minutes to get to work. What’s the distribution? Here’s a function to do the simulation, and some sample commute times: ## [1] 36.56652 33.33859 28.53662 22.88209 13.04609 24.66753 20.66765 ## [8] 18.61433 20.92518 21.87665 Here’s the density function: Sanity check: let’s see that the theoretical density matches the empirical density (after generating 2000 points): 8.5.2 Application: Clustering Consider the faithful data set that comes with the datasets package in R. There appears to be two categories of eruptions here. Can we group these into clusters? This is one topic of unsupervised learning (DSCI 563) – one idea is to fit a Gaussian mixture, such as the one depicted below underneath the data. 8.5.3 Application: Zero-Inflated Models Consider the following (made up) data of total rainfall for each day in September: Sun Mon Tue Wed Thu Fri Sat 0.0 0.0 0.0 1.7 0 11.8 3.2 1.5 2.8 0.0 0.0 0 19.8 0.0 5.0 0.0 0.2 1.6 0 1.0 5.1 0.0 3.6 0.0 0.0 0 0.0 0.0 6.3 0.0 NA NA NA NA NA This isn’t quite continuous data, because 0 mm of rain is possible. Yet, the data aren’t discrete, because there are uncountably many outcomes in the case that it is raining. The idea is to make a zero-inflation model: Rain either happens or it doesn’t; If no rain, then a value of 0 is taken; if there is rain, then a value is drawn from some distribution (like Weibull or Exponential). Since this random variable is neither continuous nor discrete, it has neither a pmf nor a pdf. But it does have a cdf. Suppose: there’s a 0.4 chance of rain, and if it is raining, then the amount of rain follows an Exponential distribution with a mean of 5mm. Then we have the following cdf: 8.5.4 Application: Bayesian Statistics Let’s return to our Mario Kart example. It turns out that the item distribution changes depending on how close you are to being in the lead. Let’s suppose these are the item distributions for getting an item when you’re in first, second, and third place (notice there are new items you can get!). Let’s call \\(f_1(x)\\), \\(f_2(x)\\), and \\(f_3(x)\\) the corresponding item pmf’s. Item Name Probability: 1st place, \\(f_1(x)\\) Probability: 2nd place, \\(f_2(x)\\) Probability: 3rd place, \\(f_3(x)\\) Banana 0.12 0.22 0.10 Bob-omb 0.05 0 0.05 Coin 0.75 0.22 0.10 Horn 0.03 0.03 0.10 Shell 0.05 0.20 0.10 Red shell 0 0.30 0.30 Mushroom 0 0.03 0.25 You’re a good player, so you never find yourself getting an item when you’re less than third place. In fact, the probability of being in first, second, and third places when getting an item are as follows: Place Probability 1st 0.7 2nd 0.2 3rd 0.1 The overall item distribution is a mixture distribution. Let \\(X\\) be the item you end up getting, \\(p_1, \\ldots, p_3\\) be the placing probabilities, and \\(f_1, \\ldots, f_3\\) be the item distributions. Then, \\[P(X = x) = p_1 f_1(x) + p_2 f_2(x) + p_3 f_3(x),\\] where \\(x\\) is one of the items (like “mushroom”, “horn”, etc.). We can also view this distribution in two steps: Simulate your placing; Simulate from the corresponding item distribution. Here’s the mixture distribution compared with the individual distributions It’s not as obvious that this is a mixture distribution when comparing to the Gaussian mixture or the zero-inflated model, but it indeed is. Let’s now consider the reverse situation. Here are 100 items that a player has collected: How often is this person in 1st place? 2nd place? 3rd place? There are more than 3 possible places, but let’s just consider three to simplify discussions. The idea is to start with prior probabilities for each placing – probably 1/3 for each place, especially if you don’t know the player. Then compute the posterior probabilities, which are the probabilities of each placing given the data. See the Appendix for Bayes’ Theorem. 8.5.5 Activity Recall the probability of getting a banana in Mario Kart based on placing: Item Name Probability: 1st place Probability: 2nd place Probability: 3rd place Banana 0.12 0.22 0.10 You’re equally likely to be in 1st, 2nd, or 3rd place. What’s the mixture distribution of the binary variable of getting a banana or not? You’re always in first place. What’s the mixture distribution of getting a banana or not? END OF QUIZ 2 MATERIAL 8.6 Topics in the Appendix Let’s use the remaining time to check out some topics in the Appendix. We’ll look at them in order of importance: Bayes’ Theorem Heavy-tailed distributions Generating Continuous data "],
["linear-algebra-review.html", "A Linear Algebra Review A.1 Review of vectors and linear algebra A.2 Random vectors", " A Linear Algebra Review For this brief linear algebra review, we’ll set up a Python workspace: import numpy as np from numpy.random import randn, rand, seed import matplotlib.pyplot as plt seed(7) A.1 Review of vectors and linear algebra A.1.1 Vectors A vector is a container that holds multiple numbers It’s written as \\[x = \\begin{pmatrix}x_1\\\\x_2\\\\ \\vdots \\\\ x_d \\end{pmatrix}\\] For example, \\(x = \\begin{pmatrix}2\\\\-1 \\\\ 5.5 \\end{pmatrix}\\) is a 3-dimensional vector because it has 3 elements This is like the math equivalent of a list or array in R/Python, but the values will be numerical To say \\(x\\) is a \\(d\\)-dimensional vector, we can write \\(x\\in \\mathbb{R}^d\\). You can think of a vector as a point in \\(d\\)-dimensional space. d = 5 x, y = randn(d),randn(d) plt.scatter(x,y); plt.xlim(-2,2); ## (-2, 2) plt.ylim(-2,2); ## (-2, 2) plt.show() Each of these 5 points is a 2D vector. They are: for i in range(d): print(&quot;Vector number %d is (%.2f,%.2f)&quot; % (i+1, x[i], y[i])) ## Vector number 1 is (1.69,0.00) ## Vector number 2 is (-0.47,-0.00) ## Vector number 3 is (0.03,-1.75) ## Vector number 4 is (0.41,1.02) ## Vector number 5 is (-0.79,0.60) We can also draw them as arrows: plt.quiver(0*x,0*y,x,y,angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1); plt.scatter(x,y); plt.xlim(-2,2); ## (-2, 2) plt.ylim(-2,2); ## (-2, 2) plt.show() A.1.2 Matrices A matrix is a 2D container that holds numbers Here is where the confusion starts… the word dimension has 2 meanings (not my fault!) We refer to the length of a vector as its dimension, because we think of it as a point in \\(d\\)-dimensional space But in terms of being a container holding numbers, it’s a 1-dimensional contained regardless of its length Make sure you understand this! (and see below) x = randn(5) x ## array([-0.62542897, -0.17154826, 0.50529937, -0.26135642, -0.24274908]) x[0] ## -0.62542897396675967 Above: we call this a 5-dimensional vector because it’s a point in 5-dimensional space x.shape ## (5,) But it’s also 1-dimensional x.ndim ## 1 It would be less confusing to call it a “vector of length 5” rather than “a vector of dimension 5” but this is how the world is, and you need to be aware &amp; able to handle it. x = randn(3,3) x ## array([[-1.45324141, 0.55458031, 0.12388091], ## [ 0.27445992, -1.52652453, 1.65069969], ## [ 0.15433554, -0.38713994, 2.02907222]]) x[2,1] ## -0.38713994328638812 x.shape ## (3, 3) x.ndim ## 2 y = randn(2,3,4) y[0,0,0] ## -0.04538602986064609 y.size ## 24 Matrices do not have to be “square”, e.g. x = rand(3,5) x ## array([[ 0.41488598, 0.00142688, 0.09226235, 0.70939439, 0.5243456 ], ## [ 0.69616046, 0.95546832, 0.68291385, 0.05312869, 0.30885268], ## [ 0.59259469, 0.23512041, 0.964971 , 0.94504822, 0.84840088]]) x.shape ## (3, 5) x.ndim ## 2 We will only deal with 1D containers (vectors) and 2D containers (matrices). We don’t touch 3D (or higher) containers; FYI these are called tensors. However, we’ll deal with vectors and matrices of various sizes! Assumed knowledge: Matrix/vector addition/subtraction Matrix/vector multiplication (ideally) Inverse of a matrix (ideally) Some geometric intuition about linear algebra A.2 Random vectors In addition to a vector containing numbers, you can also have a vector containing random variables. This is called a random vector. I will probably slip and sometimes refer to it as a multidimensional random variable. According to Vincenzo (PhD in statistics), this isn’t formally right. In my life, it has served me fine. This is convenient for representing systems with many RVs, so we don’t need to write \\(X,Y,Z,A,B,C,\\ldots\\) We can just write \\(X\\) as a \\(d\\)-dimensional vector, and its elements are the random variables "],
["bayes-theorem.html", "B Bayes’ Theorem", " B Bayes’ Theorem Brain teaser: A heritable disease occurs randomly in 10% of the population. If someone has the disease, it is passed on to their children with probability 50%. A mother has 1 healthy child. Given this, what’s the conditional probability that the mother has the disease? Is the answer 10%? Less? More? How do we quantify it? Let \\(M\\) be the event that the mother has the disease. Let \\(C\\) be the event that the child has the disease. We want \\(P(M\\mid \\textrm{not } C)\\). We have \\(P(M)=0.1\\) and \\(P(\\textrm{not }C\\mid M)=0.5\\). Solution: \\[P(M \\mid \\textrm{not } C) = \\frac{P(\\textrm{not } C \\mid M)P(M)}{P(\\textrm{not } C)}\\] So we still need \\(P(\\textrm{not } C)\\). This could happen in 2 ways (“law of total probability”) \\[P(\\textrm{not } C)=P(\\textrm{not } C \\mid M)P(M) + P(\\textrm{not } C \\mid \\textrm{not } M)P(\\textrm{not } M)\\] We know \\(P(\\textrm{not } M)=1-P(M)=0.9\\). And we assume \\(P( C \\mid \\textrm{not } M)=0.1\\) because the child can randomly get the disease like anyone else, so then \\(P(\\textrm{not } C \\mid \\textrm{not } M)=1-P( C \\mid \\textrm{not } M)=0.9\\). Finally, then, we’re left with: \\[P(M \\mid \\textrm{not } C) = \\frac{0.5 \\times 0.1}{0.5\\times 0.1 + 0.9 \\times 0.9}\\] Sanity check: this is less than 10%. That’s what our intuition told us. We can get what we need using Bayes’ Theorem. We’ve seen above that, for events \\(A\\) and \\(B\\), \\(P(A,B)=P(A\\mid B)P(B)\\). We can also write this as \\(P(A,B)=P(B\\mid A)P(A)\\). Since these are equal, we get the famous Bayes’ theorem: \\[P(A\\mid B)=\\frac{P(B\\mid A)P(A)}{P(B)}\\] (0.5*0.1)/(0.5*0.1+0.9*0.9) ## [1] 0.05813953 Follow-up question: What’s the conditional probability that the next child will have the disease? Suppose a drug test produces a positive result with probability 0.99 for drug users, \\(P(T = 1\\mid D = 1) = 0.99\\). It also produces a negative result with probability \\(0.99\\) for non-drug users, \\(P(T = 0\\mid D = 0) = 0.99\\). The probability that a random person uses the drug is \\(0.001\\), so \\(P(D = 1) = 0.001\\). What is the probability that a random person who tests positive is not actually a user, \\(P(D = 0 \\mid T = 1)\\)? "],
["heavy-tailed-distributions.html", "C Heavy-Tailed Distributions C.1 Sensitivity of the mean to extremes C.2 Heavy-tailed Distributions C.3 Heavy-tailed distribution families C.4 Extreme Value Analysis C.5 Multivariate Student’s t distributions", " C Heavy-Tailed Distributions Consider the weekly returns of the Singapore Straights (STI) market, depicted by the following histogram. You’ll notice some extreme values that are far from the “bulk” of the data. Traditional practice was to view these extremes as “outliers” that are a nuisance for analysis, and therefore should be removed. But this can actually be detrimental to the analysis, because these outliers are real occurences that should be anticipated. Instead, Extreme Value Analysis is a practice that tries to get a sense of how big and how frequently extremes will happen. C.1 Sensitivity of the mean to extremes Indeed, the empirical (arithmetic) mean is sensitive to outliers: consider the sample average of 100 observations coming from a N(0,1) distribution: set.seed(6) n &lt;- 50 x &lt;- rnorm(n) mean(x) ## [1] 0.08668773 Here’s that mean depicted on a histogram of the data: Now consider calculating the mean by replacing the last observation with 50 (a very large number): x[n] &lt;- 50 mean(x) ## [1] 1.082927 This is a big difference made by a single observation! Let’s take a look at the histogram now (outlier not shown). The “old” mean is the thin vertical line: There are robust and/or resistant ways of estimating the mean that are less sensitive to the outliers. But what’s more interesting when you have extreme values in your data is to get a sense of how frequently extremes will happen, and the mean won’t give you that sense. C.2 Heavy-tailed Distributions Distributions known as heavy-tailed distributions give rise to extreme values. These are distributions whose tail(s) decay like a power decay. The slower the decay, the heavier the tail is, and the more prone extreme values are. For example, consider the member of the Pareto Type I family of distributions with survival function \\(S(x) = 1/x\\) for \\(x \\geq 1\\). Here is this distribution compared to an Exponential(1) distribution (shifted to start at \\(x=1\\)): Notice that the Exponential survival function becomes essentially zero very quickly, whereas there’s still lots of probability well into the tail of the Pareto distribution. Also note that if a distribution’s tail is “too heavy”, then its mean will not exist! For example, the above Pareto distribution has no mean. C.3 Heavy-tailed distribution families Here are some main families that include heavy-tailed distributions: Family of Generalized Pareto distributions Family of Generalized Extreme Value distributions Family of Student’s t distributions The Cauchy distribution is a special case of this. C.4 Extreme Value Analysis There are two key approaches in Extreme Value Analysis: Model the tail of a distribution using a theoretical model. That is, choose some x value, and model the distribution beyond that point. It turns out a Generalized Pareto distribution is theoretically justified. The peaks over thresholds method models the extreme observations occurring in a defined window of time. For example, the largest river flows each year. It turns out a Generalized Extreme Value distribution is theoretically justified here. C.5 Multivariate Student’s t distributions Just like there’s a multivariate Gaussian distribution, there’s also a multivariate Student’s t distribution. And in fact, its contours are elliptical, too! Here’s a comparison of a bivariate Gaussian and a bivariate Student’s t distribution, both of which are elliptical. One major difference is that a sample from a bivariate Gaussian distribution tends to be tightly packed, whereas data from a bivariate Student’s t distribution is prone to data deviating far from the main “data cloud”. And here are samples coming from these two distributions. Notice how tightly bundled the Gaussian distribution is compared to the t distribution! "],
["generating-continuous-data.html", "D Generating Continuous Data", " D Generating Continuous Data Until now, we’ve sidestepped the actual procedure for how a random outcome is actually generated. For the discrete case, we could get by with the “drawing from a hat” analogy. But this won’t get us far in the continuous case, because each outcome has 0 probability of occuring. The idea is to convert a random number between 0 and 1 into an outcome. Going back to the discrete case, using the Mario Kart example, we can break the interval [0, 1] into sub-intervals with widths equal to their probabilities. Visually, this might look like the following: We can make a similar plot for a Poisson(3) random variable (the y-axis is truncated because we can’t plot all infinite outcomes): Indeed, this plot is nothing other than the quantile function! This idea extends to all random variables. If we want to generate an observation of a random variable \\(Y\\) with quantile function \\(Q_Y\\), just follow these two steps: Generate a number \\(U\\) completely at random between 0 and 1. Calculate the observation as \\(Y = Q_Y(U)\\). For continuous random variables only, the opposite of this result also has important implications: if \\(Y\\) is a continuous random variable with cdf \\(F_Y\\), then \\[F_Y(Y) \\sim \\text{Unif}(0,1).\\] This is important for p-values in hypothesis testing (DSCI 552+), transformations, and copulas (optional question on your lab assignment). "]
]
